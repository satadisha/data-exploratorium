{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f597209",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 2px;\">\n",
    "  \n",
    "  <div style=\"text-align: left; padding: 0;\">\n",
    "   <h2 style=\"font-size: 1.8em; margin-bottom: 0;\"><b>Moving beyond Linearity...</b></h2>\n",
    "   <br>\n",
    "   <h3 style=\" font-size: 1.2em;margin-bottom: 0;\">Logistic Regression</h3>\n",
    "   <h3 style=\"font-size: 1.2em; margin-bottom: 0; color: blue;\"><i>Dr. Satadisha Saha Bhowmick</i></h3>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"margin-right: 5px; padding: 0;\">\n",
    "    <img src=\"images/intro-pic.png\" align=\"right\" alt=\"intro-pic\" style=\"width: 70%;\">\n",
    "    <!-- TEXT NEXT TO IMAGE -->\n",
    "      <div style=\"font-size: 0.5em;\">\n",
    "        <p>Woman teaching geometry, from a fourteenth-century edition of Euclid‚Äôs geometry book.</p>\n",
    "      </div>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9557ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dea9ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Outcomes\n",
    "- Generalized Linear Models\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce0471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "We have seen how specific assumptions of linearity are broadly applicable constructs.\n",
    "\n",
    "- Can produce versatile models that can work with numerous predictors.\n",
    "- Can accommodate non linear transformations that interact linearly with each other.\n",
    "\n",
    "Not good for modeling probabilities or data with non-normal response variables!<br>($\\color{blue}{\\textbf{Recall linear regression}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7de282",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ordinary Least Squares (OLS) Assumptions\n",
    "\n",
    "Ordinary least squares (OLS) regression relies on several key assumptions:\n",
    "\n",
    "1. **The response variable is continuous and unbounded**\n",
    "   \n",
    "   $$\n",
    "   Y \\in (-\\infty, \\infty)\n",
    "   $$\n",
    "\n",
    "2. **Errors are normally distributed**\n",
    "   \n",
    "   $$\n",
    "   Y = X\\beta + \\varepsilon,\n",
    "   \\qquad\n",
    "   \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "   $$\n",
    "\n",
    "3. **Constant variance (homoscedasticity)**\n",
    "   \n",
    "   $$\n",
    "   \\mathrm{Var}(Y \\mid X) = \\sigma^2\n",
    "   $$\n",
    "\n",
    "These assumptions are often violated for many real-world outcomes, such as binary responses, count data, and proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba5ff8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What we need?\n",
    "\n",
    "A unified, consistent framework to model specific assumptions of linearity for diverse data types, where linear regression is too restrictive.\n",
    "\n",
    "Enter $\\color{blue}{\\textbf{Generalized Linear Models}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a3c27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Linear Models\n",
    "\n",
    "A generalized linear model (GLM) is a model in which: \n",
    "- a response variable is drawn from a distribution \n",
    "- its expected value $\\mathbb{E}(Y|X)$ is related to the explanatory variables through a regression equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71d3cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Linear Models\n",
    "\n",
    "There's actually $3$ components to a Generalized Linear Model.\n",
    "- <b>Random Component</b>: specifies¬†the probability distribution of the response variable. \n",
    "    - Normal distribution for $Y$ in the classical regression model\n",
    "    - Bernoulli distribution for¬†$Y$ in the binary logistic regression model\n",
    "- <b>Systematic Component</b>: specifies the explanatory variables $(X_1, X_2, \\dots, X_k)$ in the model and their optimal linear combination\n",
    "    - Linear and Logistic Regression: $\\eta = \\beta_0 + \\beta_1 X_1 + \\ldots +\\beta_kX_k $\n",
    "- <b>Link Function</b>: specifies the link between the random and the systematic component. The link function $g(\\cdot)$ indicates how the expected value of the response variable $\\mathbb{E}(Y|X)$ relates to the linear combination of explanatory variables.\n",
    "    - $g(\\mathbb{E}(Y\\mid X_1,...,X_k)) = \\eta$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a15c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Linear Model Assumptions\n",
    "\n",
    "- The data $ùëå_1, ùëå_2  \\dots ùëå_ùëõ$ are independently distributed, i.e., cases are independent.\n",
    "- The dependent variable $ùëå_ùëñ$  does <b>NOT</b> need to be normally distributed.\n",
    "- $Y$ comes from a distribution in the exponential family. (Poisson, Binomial, Normal, $\\chi^2$, Categorical, etc.).\n",
    "- Mean-Variance Relationship: In GLM, the variance depends only on the expected value.\n",
    "    - $\\textrm{Var}(Y\\mid X_1,...,X_k) = f(\\mathbb{E}(Y \\mid X_1,..,X_k))$\n",
    "- A GLM does <b>NOT</b> assume a linear relationship between the response variable and the explanatory variables.\n",
    "- A GLM <b>does</b> assume a linear relationship between the <b><i>transformed expected response</i></b> in terms of the link function and the explanatory variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04323594",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Consider the code below. \n",
    "\n",
    "Is $Y$ a generalized linear model of $X_1$ and $X_2$? What about $Z$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245f0b4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.random.normal(0,1,size=100)\n",
    "X2 = np.random.uniform(-1,1,size=100)\n",
    "errors = np.random.normal(0,1,size=100)\n",
    "Y = np.exp(X1+X2+7)+X1*errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cc3a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X1 = np.random.normal(0,1,size=100)\n",
    "X2 = np.random.uniform(-1,1,size=100)\n",
    "errors = np.random.normal(0,1,size=100)\n",
    "Z = np.exp(X1+X2+7)+(X1+X2+7)*errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030039a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS Regression\n",
    "\n",
    "A special case of Generalized Linear Models.\n",
    "- Random component - A continuous target variable $ùëå$ with a normal distribution with mean $\\mu$ and constant variance $\\sigma$.\n",
    "- Systematic component ‚Äì linear combinations of continuous or discrete explanatory variables.\n",
    "- Link function ‚Äì Identity function, $\\eta = g(\\mathbb{E}(Y)) = \\mathbb{E}(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49427423",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS Regression\n",
    "\n",
    "In ordinary least squares regression, we typically assume that the errors\n",
    "$\\varepsilon$ are independent of $X$ and have constant variance,\n",
    "$$\n",
    "\\varepsilon \\perp X,\n",
    "\\qquad\n",
    "\\mathrm{Var}(\\varepsilon \\mid X) = \\sigma^2.\n",
    "$$\n",
    "\n",
    "In generalized linear models, we relax the assumption of constant variance.\n",
    "Instead, we assume that the conditional variance of the response depends on\n",
    "the conditional mean, which is a function of $X$:\n",
    "$$\n",
    "\\mathrm{Var}(Y \\mid X) = f(\\mathbb{E}[Y \\mid X])\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ce5f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary Classification\n",
    "\n",
    "In Binary Classification, we have a binary variable $Y \\in \\{0,1\\}$, and predictors $X = (x_1,..,x_k)$. If for a given data point we have $y=1$ we say that it is in the positive class, otherwise it is in the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e369a34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bernoulli Trials\n",
    "\n",
    "If $Y \\in \\{0,1\\}$ represents success or failure for a single independent trial, then\n",
    "$$\n",
    "\\boxed{Y \\sim \\text{Bernoulli}(\\pi)}\n",
    "$$\n",
    "where\n",
    "$\n",
    "\\pi = \\mathbb{P}(Y = 1).\n",
    "$\n",
    "\n",
    "The Bernoulli distribution has:\n",
    "$$\n",
    "\\mathbb{E}[Y] = \\pi,\n",
    "\\qquad\n",
    "\\mathrm{Var}(Y) = \\pi(1-\\pi)\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, for binary classification with independent trials, the response variable $Y$ follows a Bernoulli distribution (for a single trial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba564df0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binary Classification\n",
    "\n",
    "For given values of $X$, we have that $\\mathbb{E}(Y \\mid X)$ is just the probability $\\pi$ that $Y$ is $1$.\n",
    "\n",
    "Note also for independent trials, where the conditional distribution $Y \\mid X$ follows a Bernoulli distribution with chance of success $\\pi_i$, has variance $\\pi_i(1-\\pi_i)$. Hence, <i>it only depends on $\\mathbb{E}(Y \\mid X)$</i>.\n",
    "\n",
    "Hence, we automatically satisfy $2$ out of the $3$ assumptions for a Generalized Linear Model for any binary classification with independent trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d67869",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Link Function\n",
    "A binary classification problem (with independent trials) can then be modeled as a Generalized Linear Model as long as their is a link function $g$ so that:\n",
    "$$g(\\mathbb{E}(Y \\mid X)) = \\beta_0 + \\beta_1 X_1 + \\ldots \\beta_k X_k$$\n",
    "\n",
    "This is exactly the setting of $\\color{blue}{\\textbf{Logistic Regression}}$, which models the\n",
    "<i>Bernoulli mean</i> $\\pi$ as a function of predictors via a link function.\n",
    "- <b>Log-Odds</b>: Here we use the log odds and our link functions is $g(\\pi) = \\log(\\frac{\\pi}{1-\\pi})$. (Common in Statistics/Data Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c7255",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "To round up the GLM specifications of Logistic Regression.\n",
    "\n",
    "- <b>Random component</b> ‚Äì Binary response variable that follows a Bernoulli distribution with probability of success $\\pi$\n",
    "    - Or, Binomial RV with a single trial and success probability $\\pi$.\n",
    "- <b>Systematic component</b> ‚Äì Linear combinations of continuous or discrete explanatory variables, $\\eta = \\beta_0 + \\beta_1 X_1 + \\ldots \\beta_k X_k$.\n",
    "- <b>Link function</b> ‚Äì log odds or <b>logit function</b>, $\\eta = g(\\pi) = \\log(\\frac{\\pi}{1-\\pi})$\n",
    "    - $\\pi = g^{-1}(\\eta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec80e71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "$\\color{blue}{\\textbf{Which model makes the data we actually observed most plausible?}}$\n",
    "\n",
    "<i>This is a very general procedure to get an estimator for nearly any model parameter we might be interested in.</i>\n",
    "\n",
    "- Probability asks <i>how likely is this outcome?</i>\n",
    "    - $\\mathbb{P}(\\text{data} \\mid \\theta)$\n",
    "    - The probability of observing the data given fixed model parameters~$\\theta$.\n",
    "- Likelihood asks <i>which parameter values make this observed outcome most likely?</i>\n",
    "    - Reverse the perspective, treat data as fixed\n",
    "    - Relabels this as a function of the parameters $$\\boxed{L(\\theta) = \\mathbb{P}(\\text{observed data} \\mid \\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05991337",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coin Flip Example\n",
    "\n",
    "Suppose we observe $7$ heads and $3$ tails from $10$ independent coin flips.\n",
    "If $p$ denotes the probability of heads, then the likelihood is:\n",
    "$$\n",
    "L(p) = p^{7}(1-p)^{3}\n",
    "$$\n",
    "\n",
    "Here, the data ($7$ heads, $3$ tails) are fixed, and $p$ is the quantity we vary to check at what value the likelihood is maximized.<br>\n",
    "The likelihood is maximized at $\\hat{p} = \\frac{7}{10} = 0.7$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7657b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection to Ordinary Least Squares\n",
    "\n",
    "In linear regression, we assume:\n",
    "$$\n",
    "Y_i = x_i^\\top\\beta + \\varepsilon_i,\n",
    "\\qquad\n",
    "\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "\n",
    "Under this assumption, the likelihood of the data is proportional to:\n",
    "$$\n",
    "\\exp\\!\\left(\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "If we maximize this likelihood with respect to $\\beta$ it is equivalent to minimizing the sum of squared errors $\\sum_{i=1}^n (y_i - x_i^\\top\\beta)^2$.  \n",
    "<i>Ordinary least squares is a special case of maximum likelihood estimation</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa37ed1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating Parameters for GLM via MLE\n",
    "\n",
    "For non-Gaussian data, such as binary responses, where $Y_i \\sim \\text{Bernoulli}(\\pi_i)$, the squared-error criterion is no longer appropriate.\n",
    "\n",
    "Instead, we specify the likelihood (objective function) and choose $\\beta$ that maximizes it:\n",
    "$$\n",
    "\\mathcal{L}(\\beta)\n",
    "=\n",
    "\\prod_{i=1}^n\n",
    "\\pi_i^{y_i}(1-\\pi_i)^{1-y_i},\n",
    "\\qquad\n",
    "\\pi_i = g^{-1}(x_i^\\top\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed235c60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLE for Binary Classification\n",
    "\n",
    "Because likelihoods involve products over probabilities which can cause underflow issues, we typically work with the $\\color{blue}{\\textbf{log-likelihood}}$.\n",
    "$$\n",
    "\\ell(\\beta) = - \\sum_{i=1}^n y_i \\log(\\hat{\\pi_i}) + (1-y_i) \\log(1-\\hat{\\pi_i})\n",
    "$$\n",
    "This has the same maximizer as $L(\\theta)$ but is easier to compute and differentiate. Here, $\\pi_i$ comes from the <i>Logistic model</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dd738",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logit Link Function\n",
    "\n",
    "<i>How can we make sense of the logit link function?</i>\n",
    "\n",
    "$$\n",
    "\\textit{logit}(\\pi) = \\eta; \\qquad\n",
    "\\eta = \\beta_0 + \\beta_1 X_1 + \\ldots \\beta_kX_k \n",
    "$$\n",
    "\n",
    "Also, $\\textit{logit}(\\pi) = log(\\frac{\\pi}{1-\\pi})$.\n",
    "\n",
    "With some algebraic manipulation we can derive $\\pi$ using the $\\color{blue}{\\textbf{Sigmoid}}$ function.\n",
    "$$\n",
    "\\pi = \\frac{e^{\\eta}}{1+e^{\\eta}} = \\frac{1}{1+e^{-\\eta}} = \\text{sigmoid}(\\eta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1976e83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Working with Generalized Linear Models.\n",
    "\n",
    "Much of the power with Generalized Linear Models comes from the fact that we can do much of the same things that we can do with linear models. Looking at \n",
    "\n",
    "- Cross Validation: Replace MSE or RSS, with appropriate Loss Function.\n",
    "- AIC/BIC: For Model Selection, again using Log-Likelihood formula above.\n",
    "- Regularization: ($L^1$ or $L^2$ Regularization) can be used (and in fact are used more frequently, because they improve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07098083",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thats all folks!\n",
    "\n",
    "<h4><b>For now...</b></h4>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/end-slide.jpg\" alt=\"GLM The End\" scale=\"0.01;\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
