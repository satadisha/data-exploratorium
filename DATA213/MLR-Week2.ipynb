{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dde79b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 2px;\">\n",
    "  \n",
    "  <div style=\"text-align: left; padding: 0;\">\n",
    "   <h2 style=\"font-size: 1.8em; margin-bottom: 0;\"><b>Working with many predictors and more...</b></h2>\n",
    "   <br>\n",
    "   <h3 style=\" font-size: 1.2em;margin-bottom: 0;\">An analyst's take on Linear Regression</h3>\n",
    "   <h3 style=\"font-size: 1.2em; margin-bottom: 0; color: blue;\"><i>Dr. Satadisha Saha Bhowmick</i></h3>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"margin-right: 5px; padding: 0;\">\n",
    "    <img src=\"images/intro-pic.png\" align=\"right\" alt=\"intro-pic\" style=\"width: 70%;\">\n",
    "    <!-- TEXT NEXT TO IMAGE -->\n",
    "      <div style=\"font-size: 0.5em;\">\n",
    "        <p>Woman teaching geometry, from a fourteenth-century edition of Euclid’s geometry book.</p>\n",
    "      </div>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96059a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "from ipywidgets import interact\n",
    "import sklearn.metrics as metrics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bac_data = pd.read_csv(\"BLOODALC.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d07dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Week 2: Learning Outcomes\n",
    "Moving beyond Simple Linear Regression\n",
    "\n",
    "<div style=\"display: flex; gap: 2px;\">\n",
    "\n",
    "  <div style=\"flex: 1;\">\n",
    "  <ul>\n",
    "    <li class=\"fragment\">Fitting linear models with multiple predictors.</li>\n",
    "    <li class=\"fragment\">Model Fit Statistics for evaluation</li>\n",
    "    <li class=\"fragment\">Using Fit Statistics for inference vs prediction</li>\n",
    "    <li class=\"fragment\">Linear Regression Assumptions and Diagnostics</li>\n",
    "  </ul>\n",
    "\n",
    "  \n",
    "  </div>\n",
    "\n",
    "  <div style=\"flex: 1;\">\n",
    "  <ul>\n",
    "    <li class=\"fragment\">Identify significant variables in a linear regression model.</li>\n",
    "    <li class=\"fragment\">Confidence and Prediction Intervals for Regression</li>\n",
    "    <li class=\"fragment\">Impact of Influential Observations</li>\n",
    "  </ul>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029e270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "- The world is too complex for every experiment or phenomenon to be explained by a single predictor.\n",
    "- A general linear model with multiple predictors is more common in practice.\n",
    "- General form of the equation is same as before : $ y = X\\beta + \\epsilon $\n",
    "- Normal equations still hold to provide the least-squares solution $\\hat{\\beta}$ : $X^TX\\hat{\\beta} = X^Ty$\n",
    "- Instead of a least squares line, we have a hyperplane!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31694d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "Given a collection of $p$ potential predictors, multiple linear regression fits the following model:\n",
    "$$\\begin{aligned}\n",
    "&& E(Y|X)= \\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}+ ... +\\beta_{p}X_{p}\\\\\n",
    "&& Var(Y|X)= \\sigma^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "All the $\\beta$ s and $\\sigma^2$ are unknown parameters to be estimated.\n",
    "- $\\color{blue}{\\text{Ordinary Least Squares}}$(OLS) Estimator: $ \\hat{\\beta} = (X^TX)^{-1}X^Ty $\n",
    "- For the least squares solution to exist, $X^TX$ must be invertible.\n",
    "- We still do not compute the $\\beta$ s from least squares estimates directly. \n",
    "    - Uncorrected sums of squares and cross products are prone to large rounding errors! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8a50d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Linear Regression\n",
    "\n",
    "For any observable pair of $(x,y)$ we have:\n",
    "$$\\begin{aligned}\n",
    "&& \\hat{y} = E(Y|X=x)= \\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+ ... +\\beta_{p}x_{p}\\\\\n",
    "&& \\hat{e} = y - \\hat{y}\n",
    "\\end{aligned}$$\n",
    "\n",
    "$e$ is the residual or error term.\n",
    "\n",
    "Linear regression places the following <u><b>assumptions on the error</b></u>.\n",
    "$$\\begin{aligned}\n",
    "&& E(e|X) = 0; Var (e|X) = \\mathcal{\\sigma^2 I_n}\\\\\n",
    "&& \\textbf{Normality: } P(e|X) \\sim \\mathcal{N(0, \\sigma^2 I_n)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "More on model assumptions later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740638b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression: Beers and BAC\n",
    "\n",
    "<div class=\"fragment\" style=\"\n",
    "    border: 4px solid #3b82f6;\n",
    "    background: #eef6ff;\n",
    "    padding: 10px 15px;\n",
    "    border-radius: 6px;\n",
    "    margin: 10px auto;\n",
    "    width: 90%;\n",
    "    box-sizing: border-box;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.35;\n",
    "    color: #1e40af;\">\n",
    "  \n",
    "  Researchers at Ohio State University had sixteen volunteers drink a randomly drawn number of 12 ounce beer cans. Thirty minutes after drinking their last beer, each subject had their blood alcohol content (BAC) measured. Subjects had their gender and weight recorded and were also administered a road sobriety test before and after the alcohol consumption. We want to predict <b>BAC</b> from the remaining information available for each subject.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0e4acc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_OSU</th>\n",
       "      <th>Gender_OSU</th>\n",
       "      <th>Weight_OSU</th>\n",
       "      <th>Beers</th>\n",
       "      <th>BAC</th>\n",
       "      <th>Sobr1</th>\n",
       "      <th>Sobr2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>110</td>\n",
       "      <td>9</td>\n",
       "      <td>0.190</td>\n",
       "      <td>9.75</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>192</td>\n",
       "      <td>8</td>\n",
       "      <td>0.120</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>male</td>\n",
       "      <td>172</td>\n",
       "      <td>3</td>\n",
       "      <td>0.040</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.095</td>\n",
       "      <td>9.50</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>female</td>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>0.070</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>male</td>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "      <td>0.060</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>female</td>\n",
       "      <td>175</td>\n",
       "      <td>3</td>\n",
       "      <td>0.020</td>\n",
       "      <td>9.50</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>male</td>\n",
       "      <td>275</td>\n",
       "      <td>5</td>\n",
       "      <td>0.050</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>female</td>\n",
       "      <td>130</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070</td>\n",
       "      <td>9.50</td>\n",
       "      <td>8.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>male</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>0.100</td>\n",
       "      <td>9.50</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>female</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>0.085</td>\n",
       "      <td>9.75</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>male</td>\n",
       "      <td>246</td>\n",
       "      <td>7</td>\n",
       "      <td>0.090</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>male</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>175</td>\n",
       "      <td>4</td>\n",
       "      <td>0.050</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID_OSU Gender_OSU  Weight_OSU  Beers    BAC  Sobr1  Sobr2\n",
       "0        1     female         132      5  0.100  10.00   6.00\n",
       "1        2     female         128      2  0.030   9.50   9.25\n",
       "2        3     female         110      9  0.190   9.75   4.75\n",
       "3        4       male         192      8  0.120  10.00   7.50\n",
       "4        5       male         172      3  0.040  10.00   9.75\n",
       "5        6     female         250      7  0.095   9.50   6.50\n",
       "6        7     female         125      3  0.070   9.50   7.00\n",
       "7        8       male         175      5  0.060   9.75   8.75\n",
       "8        9     female         175      3  0.020   9.50   6.00\n",
       "9       10       male         275      5  0.050   9.75   8.50\n",
       "10      11     female         130      4  0.070   9.50   8.50\n",
       "11      12       male         168      6  0.100   9.50   7.75\n",
       "12      13     female         128      5  0.085   9.75   8.25\n",
       "13      14       male         246      7  0.090  10.00   7.75\n",
       "14      15       male         164      1  0.010   9.50   9.50\n",
       "15      16       male         175      4  0.050  10.00   9.00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bac_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581e3ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Aside on Categorical Variables\n",
    "\n",
    "Our data has a categorical variable `Gender_OSU` whose values are converted to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53240f60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight_OSU</th>\n",
       "      <th>Beers</th>\n",
       "      <th>Sobr1</th>\n",
       "      <th>Sobr2</th>\n",
       "      <th>BAC</th>\n",
       "      <th>Gender_OSU_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.25</td>\n",
       "      <td>0.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>9</td>\n",
       "      <td>9.75</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>8</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "      <td>3</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>0.04</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Weight_OSU  Beers  Sobr1  Sobr2   BAC  Gender_OSU_male\n",
       "0         132      5  10.00   6.00  0.10            False\n",
       "1         128      2   9.50   9.25  0.03            False\n",
       "2         110      9   9.75   4.75  0.19            False\n",
       "3         192      8  10.00   7.50  0.12             True\n",
       "4         172      3  10.00   9.75  0.04             True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_vars = [\"Gender_OSU\", \"Weight_OSU\", \"Beers\", \"Sobr1\", \"Sobr2\", \"BAC\"]\n",
    "bac_data = pd.get_dummies(bac_data[reg_vars], drop_first = True)\n",
    "bac_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844b4a2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept                  0.092460\n",
       "Gender_OSU_male[T.True]    0.001261\n",
       "Beers                      0.019131\n",
       "Weight_OSU                -0.000349\n",
       "Sobr1                     -0.003447\n",
       "Sobr2                     -0.002320\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model using R-style formula\n",
    "# The format is 'dependent_variable ~ independent_variable1 + independent_variable2 + ...'\n",
    "mlr_formula = 'BAC ~ Beers + Weight_OSU + Gender_OSU_male + Sobr1 + Sobr2'\n",
    "\n",
    "# Fit the model using OLS (Ordinary Least Squares)\n",
    "model = smf.ols(formula=mlr_formula, data=bac_data).fit()\n",
    "\n",
    "# Print the summary of the regression results\n",
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca57ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regression Model: Beers and BAC\n",
    "\n",
    "Our linear regression model can be symbolically expressed as:\n",
    "$$ BAC = 0.092460 + 0.001261 \\times Gender\\_OSU\\_male + 0.019131 \\times Beers -0.000349 \\times Weight\\_OSU -0.003447 \\times Sobr1 -0.002320 \\times Sobr2 $$\n",
    "\n",
    "<span class=\"fragment\">\n",
    "<h4 style=\"color: blue; font-weight: bold; font-style: italic;\">Your Turn</h4>\n",
    "\n",
    "What interpretations can you make of the impact of the model variables on BAC looking at the model coefficients?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c806cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sum of Squares\n",
    "\n",
    "When discussing linear regression, it is common to have three types of sum of squares.\n",
    "- $\\color{blue}{\\textbf{SST}}$: Total variability of data. \n",
    "$\\lVert \\vec{y}-\\bar{y} \\rVert^2$\n",
    "- $\\color{blue}{\\textbf{SSR}}$: Sum of squares of Regression. This is the portion of the total variability explained by regression.\n",
    "$\\lVert \\bar{y}-X\\vec{\\beta} \\rVert^2 = \\lVert \\bar{y}-\\hat{y} \\rVert^2$\n",
    "- $\\color{blue}{\\textbf{SSE}}$: Sum of squared errors or residuals. This is the portion of the total variability that remains unexplained.\n",
    "$\\lVert \\vec{y}-X\\vec{\\beta} \\rVert^2 = \\lVert \\vec{e} \\rVert^2$\n",
    "<br>\n",
    "\n",
    "The total variability of data is the sum of the variability explained by regression and the unexplained variability that manifests as error.\n",
    "- $SST = SSR+SSE$\n",
    "- Given a constant total variability, a lower error will indicate better regression and a higher error will indicate less powerful regression.\n",
    "\n",
    "<div style=\"\n",
    "border-left: 6px solid #2563eb;\n",
    "background: #eef6ff;\n",
    "padding: 12px 16px;\n",
    "margin: 10px 0;\n",
    "border-radius: 6px;\n",
    "\">\n",
    "\n",
    "<strong>Remark.</strong>  \n",
    "Please beware of notational differences between SSE vs RSS or SSR vs ESS depending on the text you are following.<br> We will be consistent with the notations listed above!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba1935",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sum of Squares\n",
    "\n",
    "In order to center our observations around 0, we might want to subtract them from the mean $\\bar{y}$, such that:\n",
    "- $SST= \\lVert \\vec{y} \\rVert^2$\n",
    "- $SSR= \\lVert \\hat{y} \\rVert^2$\n",
    "- $SSE= \\lVert \\vec{y}-X\\vec{\\beta} \\rVert^2$\n",
    "\n",
    "<div class=\"fragment\" style=\"\n",
    "    border: 4px solid #3b82f6;\n",
    "    background: #eef6ff;\n",
    "    padding: 10px 15px;\n",
    "    border-radius: 6px;\n",
    "    margin: 10px auto;\n",
    "    width: 90%;\n",
    "    box-sizing: border-box;\n",
    "    font-size: 1.2em;\n",
    "    line-height: 1.35;\n",
    "    color: #1e40af;\">\n",
    "\n",
    "<b>Show that:</b><br><br>\n",
    "a. $SST = SSR + SSE$ <br>\n",
    "b. $SSR = \\vec{\\beta}^TX^T\\vec{y}$ and $SSE = \\vec{y}^T\\vec{y} - \\vec{\\beta}^TX^T\\vec{y}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc2925",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A geometric approach\n",
    "\n",
    "Recall from our lessons on the way to deriving the normal equations that our least square solution $\\hat{y}=X\\vec{\\beta}$ is an orthogonal projection of $\\vec{y}$ onto the column space of the design matrix $X$. And by definition, $\\vec{y}-X\\vec{\\beta}$ is its orthogonal component.\n",
    "\n",
    "By virtue of the <b>Pythagorean Theorem</b> we have, $ \\lVert \\vec{y} \\rVert^2 = \\lVert \\hat{y} \\rVert^2 + \\lVert \\vec{y}-X\\vec{\\beta} \\rVert^2$.\\\n",
    "Or, $SST = SSR + SSE$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37ef2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An algebraic approach\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\color{red}{SSR + SSE} &= \\lVert \\hat{y} \\rVert^2 + \\lVert \\vec{y}-X\\vec{\\beta} \\rVert^2 = \\lVert X\\vec{\\beta} \\rVert^2 + \\lVert \\vec{y}-X\\vec{\\beta} \\rVert^2 \\\\\n",
    "&= X\\vec{\\beta} \\cdot X\\vec{\\beta} + (\\vec{y}-X\\vec{\\beta}) \\cdot (\\vec{y}-X\\vec{\\beta}) \\\\\n",
    "&= X\\vec{\\beta} \\cdot X\\vec{\\beta} + \\vec{y} \\cdot \\vec{y} - \\vec{y} \\cdot X\\vec{\\beta} - X\\vec{\\beta} \\cdot \\vec{y} + X\\vec{\\beta} \\cdot X\\vec{\\beta} \\\\\n",
    "&= \\lVert \\vec{y} \\rVert^2 + 2X\\vec{\\beta} \\cdot X\\vec{\\beta} - 2 \\vec{y} \\cdot X\\vec{\\beta} ; (\\textit{commutative property of dot product}) \\\\\n",
    "&= \\lVert \\vec{y} \\rVert^2 - 2 X\\vec{\\beta} \\cdot [\\vec{y} - X\\vec{\\beta}] \\\\\n",
    "&= \\lVert \\vec{y} \\rVert^2 - 2 \\cdot 0; ([\\vec{y} - X\\vec{\\beta}] \\perp X\\vec{\\beta})\\\\\n",
    "&= \\lVert \\vec{y} \\rVert^2 = \\color{red}{SST}\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0806a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics\n",
    "\n",
    "How well does our estimated regression equation fit the data?\n",
    "\n",
    "- We will study metrics that help us evaluate the fit of a regression models.\n",
    "- We will learn about metrics that help us draw statistical inferences around our model's fit.\n",
    "- We will test existing packages that help us draw both predictions and inferences from our fitted model.\n",
    "    - For predictive purposes `sklearn` is the standard Python package to use.\n",
    "    - We can use the `statsmodels` package to organize all relevant information for statistical inference in an ANOVA table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf3a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: $r^2$\n",
    "\n",
    "You may have already seen this before (DATA 119)!\n",
    "\n",
    "$\\color{blue}{\\text{Coefficient of Determination}}$: measures what proportion of the total variability of the response variable is explained by the model.\n",
    "- $ r^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$\n",
    "- always falls between 0 and 1, with values closer to 1 representing better model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f93430f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>BAC</td>       <th>  R-squared:         </th> <td>   0.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 20 Jan 2026</td> <th>  Prob (F-statistic):</th> <td>2.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:37:50</td>     <th>  Log-Likelihood:    </th> <td>  52.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    16</td>      <th>  AIC:               </th> <td>  -93.08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    10</td>      <th>  BIC:               </th> <td>  -88.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       BAC        & \\textbf{  R-squared:         } &    0.955  \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &    0.932  \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &    42.39  \\\\\n",
       "\\textbf{Date:}             & Tue, 20 Jan 2026 & \\textbf{  Prob (F-statistic):} & 2.06e-06  \\\\\n",
       "\\textbf{Time:}             &     00:37:50     & \\textbf{  Log-Likelihood:    } &   52.538  \\\\\n",
       "\\textbf{No. Observations:} &          16      & \\textbf{  AIC:               } &   -93.08  \\\\\n",
       "\\textbf{Df Residuals:}     &          10      & \\textbf{  BIC:               } &   -88.44  \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &           \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &           \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9ee3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An aside on Degrees of Freedom\n",
    "\n",
    "You may have seen this concept come up often in statistics and data science. \n",
    "Generally speaking, degrees of freedom represents the number of independent values that are free to vary during the calculation of a statistic.\n",
    "\n",
    "$\\color{blue}{\\textbf{Degrees of Freedom for Sample Variance}}$. For a dataset with n observations $Y = {y_1, y_2,..., y_n}$, sample variance calculation uses the sample mean, $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i}$. Variance also requires calculating the squared terms $(y_{i}-\\bar{y})^2; \\forall i$. However, knowing the first $n-1$ $y$-terms implies that we can infer the last term $y_n = n\\bar{y} - \\sum_{i=1}^{n-1}y_{i}$, thereby no longer making it a free (to vary) variable. Hence the calculation of sample variance has a degree of freedom of $n-1$. You may have already seen this in the formula for sample variance $\\frac{1}{n-1}\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2$, where the sum of squares is divided by the degrees of freedom instead of the number of samples!\n",
    "\n",
    "<div style=\"\n",
    "border-left: 6px solid #2563eb;\n",
    "background: #eef6ff;\n",
    "padding: 12px 16px;\n",
    "margin: 10px 0;\n",
    "border-radius: 6px;\n",
    "font-size: 0.8em;\n",
    "\">\n",
    "<b>Remark.</b>  \n",
    "Dividing the sum of squares by $n-1$, instead of the sample size $n$, to calculate the sample variance is called <b>Bessel's correction</b>.<br> This is a necessary statistical adjustment that avoids potential underestimation and ensures that the sample variance provides an unbiased estimate of the population variance.<br> This is especially crucial for small samples where the correction makes a significant difference. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a14e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Degrees of Freedom in Regression\n",
    "\n",
    "In linear regression we are dealing with two types of degrees of freedom.\n",
    "\n",
    "1. $\\color{blue}{\\textbf{Total Degrees of Freedom}}$: You can think of it as the degrees of freedom for SST. Follows from the previous discussion that this is given by $n-1$, with $n$ being the number of observations.\n",
    "2. $\\color{blue}{\\textbf{Residual Degrees of Freedom}}$: You can think of it as the degrees of freedom for SSE, i.e. the sum of squared residuals. For a regression model with $p$ coefficients (hyperplane of $p$ independent vectors), the residual degrees of freedom is given by $n-p-1$.\n",
    "\n",
    "Remember, $p$ is the $\\color{blue}{\\textbf{number of predictors}}$ used in your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca23450",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: Adjusted $r^2$\n",
    "\n",
    "A fairly big disadvantage of $r^2$ is the fact that it is non-decreasing. Given that $r^2$ is a measure of fit where model coefficients are calculated using OLS (by minimizing the SSE), its mathematical form ensures that adding more variables will still result in the best possible fit with those variables. \n",
    "\n",
    "Mathematically, adding more variables cannot increase the sum of squared residuals because the new model has more degrees of freedom to minimize these residuals. Even by adding unrelated variables, SSE can stay the same (when the corresponding coefficients are set to 0). Or worse, even decrease variation of the response variable by capturing correlations with new predictors that exist purely due to random chance.\\\n",
    "Reminder, SSE is inversely related to $r^2$.\n",
    "\n",
    "Since $r^2$ does not guarantee adding more variables will always lead to models with more explanatory power, we need an alternative!\n",
    "\n",
    "Adjusted $r^2$ factors in a model's degrees of freedom (higher for those with more variables). It is bounded between 0 and 1, with values closer to 1 being more desirable.\n",
    "$$\n",
    "r^2_{adj} = 1- \\frac{SSE}{SST}\\frac{n-1}{n-p-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d829cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>BAC</td>       <th>  R-squared:         </th> <td>   0.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 20 Jan 2026</td> <th>  Prob (F-statistic):</th> <td>2.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:37:50</td>     <th>  Log-Likelihood:    </th> <td>  52.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    16</td>      <th>  AIC:               </th> <td>  -93.08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    10</td>      <th>  BIC:               </th> <td>  -88.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       BAC        & \\textbf{  R-squared:         } &    0.955  \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &    0.932  \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &    42.39  \\\\\n",
       "\\textbf{Date:}             & Tue, 20 Jan 2026 & \\textbf{  Prob (F-statistic):} & 2.06e-06  \\\\\n",
       "\\textbf{Time:}             &     00:37:50     & \\textbf{  Log-Likelihood:    } &   52.538  \\\\\n",
       "\\textbf{No. Observations:} &          16      & \\textbf{  AIC:               } &   -93.08  \\\\\n",
       "\\textbf{Df Residuals:}     &          10      & \\textbf{  BIC:               } &   -88.44  \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &           \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &           \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07273c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: RMSE\n",
    "\n",
    "Root Mean Squared Error (RMSE) is an alternative simple metric to compare regression models. It is the square root of the average squared error (residual) of predictions made by a model.\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{SSE}{n}} = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i})^2}{n}}\n",
    "$$\n",
    "\n",
    "There is no scale for RMSE! You have to compare it to the RMSE for another model fit from the same data. Smaller values are more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028f006e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009071924032251964"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating predicted values and RMSE\n",
    "fitted2 = model.predict()\n",
    "np.sqrt(metrics.mean_squared_error(bac_data[\"BAC\"], fitted2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b3047",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: RSE\n",
    "\n",
    "This metric can go by a few different names:\n",
    "- Residual Standard Error (RSE)\n",
    "- Standard Error of the Regression\n",
    "- Standard Error of Fit\n",
    "\n",
    "It is an <u><b>unbiased estimate</b></u> of the $\\color{blue}{\\textbf{standard deviation of the error term}}$ in a regression model. I.e., it is the square root of the variance of estimated residuals.<br> \n",
    "As explained before, when working with sample data, the calculation is adjusted for the number of degrees of freedom.\n",
    "\n",
    "$$\n",
    "se = \\sqrt{\\frac{SSE}{n-p-1}} = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(y_i - \\hat{y_i})^2}{n-p-1}}\n",
    "$$\n",
    "\n",
    "It is the average distance between an observation from a sample of data and the regression hyperplane (line in 2D) in units of the response variable.\n",
    "- Think of it as a measure showing how tightly the observations surround the regression hyperplane.\n",
    "- Can be used to make prediction intervals, more on this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368a7d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RSE in practice\n",
    "\n",
    "Like RMSE, there is no scale for RSE! You have to compare it to the RSE for another model fit from the same data. Smaller values are more desirable.\n",
    "\n",
    "While the RSE is not directly available in the summary table of `statsmodel` it is defined within the package as root mean squared error of the residuals. \n",
    "- <i>Please beware that this is <u><b>not</b></u> the RMSE which is a regular average of mean squared error and not corrected with division by the appropriate degrees of freedom.</i> This can be confusing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57e5fb6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01147517708077401\n"
     ]
    }
   ],
   "source": [
    "rse = np.sqrt(model.mse_resid)\n",
    "print(rse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c9ba8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: AIC\n",
    "\n",
    "Instead of distances, if we had a likelihood function. Unlike $r^2$ and adjusted $r^2$, the Akaike Information Criterion is related to the likelihood of the data.\n",
    "$$\n",
    "AIC = 2p - 2 ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "- The $2p$ penalty discourages overfitting relative to maximizing \"goodness of fit\" as measured by $\\hat{L}$, the likelihood function of the data. Technically, we minimize the negative log likelihood.\n",
    "\n",
    "- This formula is derived from estimated Kullback-Leibler divergence – think of it as measuring information lost from a perfect model.\n",
    "\n",
    "- There is no “scale” for AIC! You have to compare it to the AIC for another model fit from the same data. Smaller values are more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e698df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-93.07623592630601"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45e1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Fit Statistics: BIC\n",
    "\n",
    "Bayesian Information Criterion (BIC) is similar to AIC but takes a stricter approach to model complexity.\n",
    "$$\n",
    "AIC = 2p ln(n) - 2 ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "- The penalty term grows with the number of observations in the dataset. BIC always prefers the simplest possible models and becomes more cautious about adding new parameters as the dataset size grows.\n",
    "\n",
    "- There is no “scale” for BIC! You have to compare it to the BIC for another model fit from the same data. Smaller values are more desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd59042",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-88.44070359286732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fc716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting and Model Selection\n",
    "\n",
    "- Most of these model fit statistics are calculated using \"in sample\" data. \n",
    "- Because of overfitting, we are often more concerned with how the model might perform on unseen test data. To simulate that, we could estimate test error on a new dataset, or a portion of the original dataset.\n",
    "    - Pros: do not have to have two datasets.\n",
    "    - Cons: You’re not necessarily using all of the data that was collected to fit your models–and in general, more data is helpful.\n",
    "\n",
    "<b>Cross validation is an important tool to combat overfitting when working with limited data. It can help address the challenges that arise in training with splitting your data.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd9a00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross Validation\n",
    "\n",
    "- Cross validation (CV) is often used in multiple contexts.\n",
    "    - As a method of model selection/evaluating model fit\n",
    "    - Also as a method of choosing hyperparameters for regularization (later).\n",
    "\n",
    "- The basic idea is to split the whole data into small portions. Fit the model with data from all but one portion and evaluate it by evaluating the goodness of fit on the unused portion. We then put this portion of data back in, and repeat the whole split-fit-predict process. Note that any of the aforementioned model fit statistic can be used to evaluate the fit of a model.\n",
    "\n",
    "- Depending on the type of data partitioning we have,\n",
    "    - Leave-One-Out Cross Validation\n",
    "    - $K$-fold Cross Validation\n",
    "\n",
    "- There is no scale for CV (of any kind). You have to compare it to the cross-validation error for another model fit from the same data. Smaller values of cross-validation error imply the predictions were close to the actual values, which is more desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97648af6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cross Validation in Practice\n",
    "\n",
    "We can use the popular `scikit-learn` library's built in model selection modules to implement cross validation. More on this [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b90731",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9382031  0.73119086 0.11008873 0.74181497 0.94173008]\n",
      "0.692605548701853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X1 = bac_data[[\"Beers\", \"Weight_OSU\"]]\n",
    "y = bac_data[[\"BAC\"]]\n",
    "\n",
    "#fitting a Linear Regression model with default 5 fold cross-validation\n",
    "print(cross_val_score(LinearRegression(fit_intercept = True), X1, y))\n",
    "print(cross_val_score(LinearRegression(fit_intercept = True), X1, y).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731ba8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variable Significance\n",
    "\n",
    "Regression Analysis can help us understand the statistical significance of variables in an experiment as potential predictors of the response.\n",
    "\n",
    "A typical framing: <i>Is the number of beers a student drinks a statistically significant predictor of the student’s blood alcohol content, all other factors held equal?</i>\n",
    "- To answer this we estimate the statistical significance of the regression model's coefficient associated with this variable.\n",
    "- We conduct a hypothesis test at a certain pre-determined significance level ($\\alpha = 5\\%$).\n",
    "    - Is the predictor's model coefficient (statistically) significantly different from 0?\n",
    "    - Null Hypothesis is assumption of no significance $H_0: \\beta_{Beers} = 0$.\n",
    "    - Alternative Hypothesis $H_A: \\beta_{Beers} \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162c1bb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variable Significance\n",
    "\n",
    "We can use the $\\color{blue}{\\textbf{Standard Error of Model Coefficients}}$ and known sampling distributions to develop inference around our model predictors.\n",
    "- Remember that the regression model is fitted around a sample. So much like sample mean, the model coefficients (parameters) are also estimates that are subject to an underlying sampling distribution(how they'd vary across samples).\n",
    "\n",
    "For a linear model, the OLS estimator is $\\hat{\\beta} = (X^TX)^{-1}X^Ty$.\\\n",
    "By substituting $y = X\\beta + \\epsilon$ we get, $\\hat{\\beta} = \\beta + (X^TX)^{-1}X^T\\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e585832",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variable Significance\n",
    "\n",
    "Turns out that upon deriving the OLS estimator:\n",
    "- Estimates of individual model coefficients are normally distributed\n",
    "    - $\\hat{\\beta}_j|X \\sim \\mathcal{N}(\\beta_j, \\sigma^2(X^TX)^{-1}_{jj})$\n",
    "- Standardized coefficients individually follow a Student's T Distribution. Also called the observed $t$-statistics of the model coefficients.\n",
    "    - $ \\frac{\\hat{\\beta}_j-\\beta_j}{se(\\hat{\\beta}_j)} \\sim \\mathcal{t}_{n-p-1}$; $\\beta_j$ reflects your prior belief in the predictor's coefficient. <u style=\"color: blue; font-size: 1.0em; font-weight: bold; font-style: italic;\">Typically set to 0</u>.\n",
    "    - The standard error of model coefficient $\\hat{\\beta}_j$ is $se(\\hat{\\beta}_j) = \\sqrt{se^2 (X^TX)^{-1}_{jj}}$, where $se$ is the standard error of regression.\n",
    "    - For simple linear regression, the standard error of the slope is given by $se(\\hat{\\beta}_1) = \\sqrt{ \\frac{se^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e6fb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variable Significance with Hypothesis Testing\n",
    "\n",
    "It’s good practice to follow these steps for any hypothesis test:\n",
    "\n",
    "- Identify sample statistics, in this case model coefficient of the predictor of interest, and determine what significance level you wish to use.\n",
    "- Check if model (linear regression) conditions hold. More on that later!\n",
    "- Compute the standard error of model coefficient and corresponding $t$-statistic. [Calculate the two-tailed $p$-value](https://www.graphpad.com/quickcalcs/pvalue1/) for this $\\lvert t \\rvert $-statistic.\n",
    "    - Alternatively, we can also refer to the $t$-critical value of a two-tailed $t$ test at the desired significance level.\n",
    "    - We must factor in the correct number of degrees of freedom!\n",
    "\n",
    "We will use the `statsmodels` output directly to find this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d367c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Variable Significance in Practice\n",
    "\n",
    "- The $\\lvert t \\rvert $-statistic statistic for $\\beta_{Beers}$ is 9.037(very high), and the corresponding $p$ value is rounded to 0.000\n",
    "- This $p$ value is lower than any reasonable significance level including $\\alpha=0.05$. Hence we <u>reject</u> the null hypothesis.\n",
    "- We conclude that there is statistically significant evidence that the number of beers a student drinks can help predict the student’s BAC.\n",
    "\n",
    "<span class=\"fragment\">\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold; font-style: italic;\">Your Turn</u><br>\n",
    "Can you comment on the significance of the other predictors?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b781f747",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>               <td>    0.0925</td> <td>    0.159</td> <td>    0.580</td> <td> 0.575</td> <td>   -0.263</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gender_OSU_male[T.True]</th> <td>    0.0013</td> <td>    0.010</td> <td>    0.133</td> <td> 0.897</td> <td>   -0.020</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beers</th>                   <td>    0.0191</td> <td>    0.002</td> <td>    9.037</td> <td> 0.000</td> <td>    0.014</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight_OSU</th>              <td>   -0.0003</td> <td> 7.45e-05</td> <td>   -4.682</td> <td> 0.001</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr1</th>                   <td>   -0.0034</td> <td>    0.016</td> <td>   -0.209</td> <td> 0.838</td> <td>   -0.040</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr2</th>                   <td>   -0.0023</td> <td>    0.004</td> <td>   -0.639</td> <td> 0.537</td> <td>   -0.010</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                 &       0.0925  &        0.159     &     0.580  &         0.575        &       -0.263    &        0.448     \\\\\n",
       "\\textbf{Gender\\_OSU\\_male[T.True]} &       0.0013  &        0.010     &     0.133  &         0.897        &       -0.020    &        0.022     \\\\\n",
       "\\textbf{Beers}                     &       0.0191  &        0.002     &     9.037  &         0.000        &        0.014    &        0.024     \\\\\n",
       "\\textbf{Weight\\_OSU}               &      -0.0003  &     7.45e-05     &    -4.682  &         0.001        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{Sobr1}                     &      -0.0034  &        0.016     &    -0.209  &         0.838        &       -0.040    &        0.033     \\\\\n",
       "\\textbf{Sobr2}                     &      -0.0023  &        0.004     &    -0.639  &         0.537        &       -0.010    &        0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0cbb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Confidence Intervals for Model Coefficients\n",
    "\n",
    "- Alternatively we can draw the 95\\% confidence interval (CI) of the model coefficient in question.\n",
    "    - As we know, confidence intervals are an effective way of capturing the uncertainty around a sample estimate.\n",
    "    - A 95\\% confidence interval means that if we repeated drawing samples from the population data and also conducted regression analysis many times, the true (population) coefficient for the predictor will be present within 95\\% of the calculated intervals. \n",
    "\n",
    "- If the interval does not contain 0 then we can infer that the predictor, whose coefficient we have been analyzing, has statistical significance in predicting the response variable.\n",
    "\n",
    "- The `statsmodels` summary table output also provides us with the 95\\% confidence intervals of each predictor. However, we can also draw intervals upon setting the significance level as we see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d1f82",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Confidence Intervals for Model Coefficients\n",
    "\n",
    "<span class=\"fragment\">\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold; font-style: italic;\">Your Turn</u><br>\n",
    "Do the CIs drawn for each predictor back up your previous inference regarding their statistical significance to predict BAC?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc8092d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-0.262734</td>\n",
       "      <td>0.447653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_OSU_male[T.True]</th>\n",
       "      <td>-0.019945</td>\n",
       "      <td>0.022467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beers</th>\n",
       "      <td>0.014414</td>\n",
       "      <td>0.023847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight_OSU</th>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sobr1</th>\n",
       "      <td>-0.040135</td>\n",
       "      <td>0.033241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sobr2</th>\n",
       "      <td>-0.010411</td>\n",
       "      <td>0.005771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0         1\n",
       "Intercept               -0.262734  0.447653\n",
       "Gender_OSU_male[T.True] -0.019945  0.022467\n",
       "Beers                    0.014414  0.023847\n",
       "Weight_OSU              -0.000515 -0.000183\n",
       "Sobr1                   -0.040135  0.033241\n",
       "Sobr2                   -0.010411  0.005771"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conf_int(alpha=0.05, cols = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc84262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty in Predictions\n",
    "\n",
    "As mentioned before, regression models are fitted on a sample of data drawn from an underlying population. As such, not only the model coefficients, but also the prediction estimates are subject to uncertainty.<br>\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Regression is not deterministic!</u>\n",
    "\n",
    "Sources of uncertainty in prediction.\n",
    "\n",
    "1. Uncertainty in predictor values: Data sampled from a population of North American adults to estimate BAC might be very different from those sampled from a population of adults in the Indian subcontinent.\n",
    "    - Out-of-Distribution data or Selection Bias.\n",
    "2. Uncertainty in coefficient values: This points to your inherent sampling variability even when pulling data from fixed populations. We have discussed this already.\n",
    "3. Variation in the actual outcome even given perfect knowledge of predictors/coefficients.\n",
    "    - For example, two individuals with the same weight and gender, drawn from the same population, drinking the same number of beers over the same time period may get two different BACs due to a variety of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f39356",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Uncertainty in Predictions\n",
    "\n",
    "In addition to reporting confidence intervals for model coefficients $\\beta$, we might want to draw similar inferences around model predictions.\n",
    "\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Reminder</u><br>\n",
    "What we estimate as prediction $\\hat{y}$ is the expected value of the response variable given the observation of predictors $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5eeb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prediction with Linear Regression\n",
    "\n",
    "For any given observation, the regression model produces the expected value of a distribution of possible responses modelled by factoring in Gaussian noise.\n",
    "\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"images/lin-reg-gaussian.png\" style=\"width:45%; margin:5px;\">\n",
    "    <img src=\"images/lin-reg-gaussian2.png\" style=\"width:45%; margin:5px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc5e89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confidence Intervals for the Mean Response\n",
    "\n",
    "As mentioned before, we can use the regression model to estimate $E(y_h|\\vec{x}_h) = \\vec{x}_h\\beta$. Much like before, we can calculate the standard error of the expected prediction and use this to generate a $t$-interval.\n",
    "\n",
    "The general formula is: $\\text{sample estimate} \\pm (\\text{t-multiplier} \\times \\text{standard error of expected prediction})$\n",
    "\n",
    "Standard error of the fit: $SE[\\bar{y_h}] = \\sqrt{se^2(\\vec{x}_h^T(X^TX)^{-1}\\vec{x}_h)}$\n",
    "- For Simple Linear Regression, $SE[\\bar{y_h}] = se\\sqrt{\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}}$\n",
    "\n",
    "Confidence Interval: $\\bar{y_h} \\pm t_{\\frac{\\alpha}{2},n-p-1}\\times SE[\\bar{y_h}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f598900",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confidence Intervals for the Mean Response\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 5px;\">\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <p>Standard error grows for values of $\\vec{x}_h$ further away from $\\bar{X}$.</p>\n",
    "    <p>Estimated variance of $y_h$ is at the minimum at the mean of the independent variable</p>\n",
    "    <p>Predictions near $\\bar{x}$ have the narrowest confidence band and the corresponding predictions are the most stable. Confidence bands flare out further away from the mean of the independent variable.</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <img src=\"images/LinRegCIBand.png\" alt=\"Confidence Band\" scale=\"0.45;\" style=\"width: 90%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc69a3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Intervals of an Individual Response\n",
    "\n",
    "Prediction intervals are uncertainty around an individual response value $\\hat{y_h}$, for a given observation $\\vec{x}_h$, rather than a statistic like the mean response or a model coefficient.\n",
    "- Gives us the range of likely predicted values for a given observation.\n",
    "\n",
    "Much like before, the general formula is: $\\text{Sample estimate} \\pm (\\text{t-multiplier} \\times \\text{standard error})$\n",
    "\n",
    "Standard error of the prediction: $SE[\\hat{y_h}] = \\sqrt{se^2(1+\\vec{x}_h^T(X^TX)^{-1}\\vec{x}_h)}$\n",
    "- For Simple Linear Regression, $SE[\\hat{y_h}] = se\\sqrt{1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}}$\n",
    "\n",
    "Confidence Interval: $\\hat{y_h} \\pm t_{\\frac{\\alpha}{2},n-p-1}\\times SE[\\hat{y_h}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15eeb6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prediction Intervals of an Individual Response\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 5px;\">\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <p>Prediction intervals are typically <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">wider</u> than confidence intervals. Using a confidence interval for a single predicted value (not a statistic) will underestimate the uncertainty.</p>\n",
    "    <p>Confidence Intervals aim to capture the variation in mean response due to sampling variability. Prediction Intervals also factor in the random error ($\\epsilon$) that can manifest in individual response values due to the inherent variability of individual observations.</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <img src=\"images/LinRegPIBand.png\" alt=\"Confidence and Prediction Intervals\" scale=\"0.45;\" style=\"width: 90%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3b481f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Confidence and Prediction Intervals in Practice \n",
    "\n",
    "- We do not have to use these formula to calculate the intervals by hand. Instead, we can reliably use statistical packages like `statsmodels`.\n",
    "\n",
    "- `summary_table` gives a lot of information, pretty much all that could reasonably be of an analyst's interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c27007c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <th>Obs</th>   <th>Dep Var</th>        <th>Predicted</th>             <th>Std Error</th>              <th>Mean ci</th>               <th>Mean ci</th>            <th>Predict ci</th>            <th>Predict ci</th>             <th>Residual</th>              <th>Std Error</th>             <th>Student</th>               <th>Cook's</th>        \n",
       "</tr>\n",
       "<tr>\n",
       "    <th></th>   <th>Population</th>         <th>Value</th>             <th>Mean Predict</th>             <th>95% low</th>               <th>95% upp</th>              <th>95% low</th>               <th>95% upp</th>                  <th></th>                  <th>Residual</th>             <th>Residual</th>                  <th>D</th>          \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>1.0</td>     <td>0.1</td>     <td>0.09369968407159773</td>  <td>0.008102117681094477</td>   <td>0.07564704088315906</td>   <td>0.11175232726003641</td>  <td>0.06240055834013381</td>   <td>0.12499880980306166</td>  <td>0.0063003159284022725</td> <td>0.008126215485502302</td>  <td>0.7753075142595526</td>     <td>0.099590327560833</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>2.0</td>    <td>0.03</td>    <td>0.031885710092675226</td>  <td>0.006921636628078912</td>   <td>0.01646334260246901</td>   <td>0.04730807758288144</td> <td>0.0020262599224550107</td>  <td>0.06174516026289544</td> <td>-0.0018857100926752274</td> <td>0.009152629973070993</td> <td>-0.20602931596966031</td>  <td>0.004046062370333531</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>3.0</td>    <td>0.19</td>     <td>0.18165420507660063</td>  <td>0.008662533046744266</td>   <td>0.16235287863871953</td>   <td>0.20095553151448173</td>  <td>0.1496186225514376</td>    <td>0.21368978760176366</td>  <td>0.008345794923399374</td>  <td>0.007525969057150357</td>  <td>1.1089329307659201</td>    <td>0.2715340282706297</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>4.0</td>    <td>0.12</td>     <td>0.1279528395045604</td>   <td>0.006135514724779922</td>   <td>0.1142820607694753</td>    <td>0.1416236182396455</td>   <td>0.09895926199697205</td>   <td>0.15694641701214876</td>  <td>-0.00795283950456041</td>  <td>0.009697172170129284</td>  <td>-0.820119449777118</td>    <td>0.04487606469975594</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>5.0</td>    <td>0.04</td>     <td>0.03405294374859834</td>  <td>0.006463713038543853</td>   <td>0.01965089359946643</td>  <td>0.048454993897730246</td> <td>0.004707477421361444</td>   <td>0.06339841007583523</td>  <td>0.0059470562514016626</td>  <td>0.00948156647345149</td>  <td>0.6272229665903306</td>   <td>0.030471694144001856</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>6.0</td>    <td>0.095</td>    <td>0.09138271560132372</td>  <td>0.008794259959050447</td>   <td>0.07178788331228382</td>   <td>0.11097754789036361</td>  <td>0.0591694450765769</td>    <td>0.12359598612607053</td>  <td>0.003617284398676285</td>   <td>0.00737161317540219</td>  <td>0.49070458698871267</td>   <td>0.05711662119644293</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>7.0</td>    <td>0.07</td>     <td>0.0572825208009107</td>   <td>0.005570586735795068</td>  <td>0.044870480066645166</td>   <td>0.06969456153517624</td> <td>0.028860766797085678</td>   <td>0.08570427480473572</td>  <td>0.012717479199089304</td>   <td>0.01003236026337298</td>  <td>1.2676457847630722</td>    <td>0.08257341110055803</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>8.0</td>    <td>0.06</td>     <td>0.07444992964857097</td>  <td>0.004655744136112865</td>   <td>0.06407628525408995</td>   <td>0.08482357404305199</td>  <td>0.04685736102062302</td>   <td>0.10204249827651893</td>  <td>-0.014449929648570975</td> <td>0.010488266566700707</td>  <td>-1.3777233403323472</td>  <td>0.062336664150820884</td> \n",
       "</tr>\n",
       "<tr>\n",
       "   <td>9.0</td>    <td>0.02</td>     <td>0.04216983066677332</td>  <td>0.008192188322617143</td>  <td>0.023916497582536587</td>   <td>0.06042316375101006</td> <td>0.010754523662878407</td>   <td>0.07358513767066824</td>  <td>-0.022169830666773322</td> <td>0.008035405373837493</td>  <td>-2.759018323948678</td>    <td>1.3186884176759859</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>10.0</td>    <td>0.05</td>     <td>0.04016431941021288</td>  <td>0.0070397527400107305</td> <td>0.024478772821968343</td>   <td>0.05584986599845742</td> <td>0.010168091026683469</td>   <td>0.0701605477937423</td>   <td>0.009835680589787121</td>  <td>0.009062095254113839</td>  <td>1.0853649530247549</td>    <td>0.11848350762733904</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>11.0</td>    <td>0.07</td>     <td>0.07118958726881228</td>  <td>0.006357304394839466</td>   <td>0.05702463035290301</td>   <td>0.08535454418472155</td>  <td>0.04195975063063628</td>   <td>0.10041942390698827</td>  <td>-0.00118958726881227</td>  <td>0.009553238710849633</td> <td>-0.12452188255918416</td>  <td>0.0011444184707134649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>12.0</td>     <td>0.1</td>     <td>0.09920293411798915</td>  <td>0.008063567803063934</td>   <td>0.08123618541052884</td>   <td>0.11716968282544947</td>  <td>0.06795327164650167</td>   <td>0.13045259658947664</td>  <td>0.0007970658820108528</td> <td>0.008164469567615022</td>  <td>0.09762616853548871</td>  <td>0.0015494578801624889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>13.0</td>    <td>0.085</td>    <td>0.09073574642130422</td>  <td>0.006566519363972104</td>   <td>0.07610462950425788</td>   <td>0.10536686333835056</td>  <td>0.0612771835285701</td>    <td>0.12019430931403834</td>  <td>-0.005735746421304214</td> <td>0.009410659513429467</td>  <td>-0.6094946282052842</td>  <td>0.030145247353949547</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>14.0</td>    <td>0.09</td>     <td>0.08941481364275149</td>   <td>0.0062005610211589</td>    <td>0.07559910272752794</td>   <td>0.10323052455797503</td> <td>0.060352618034747035</td>   <td>0.11847700925075594</td>   <td>0.00058518635724851</td>  <td>0.009655709816373214</td>  <td>0.06060521374163585</td> <td>0.00025244239102058935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <td>15.0</td>    <td>0.01</td>    <td>0.0008846011731687984</td> <td>0.007592623790715615</td>  <td>-0.016032818883277972</td> <td>0.017802021229615567</td> <td>-0.029773781804919196</td> <td>0.031542984151256795</td>  <td>0.009115398826831201</td>  <td>0.008604170675188887</td>   <td>1.059416319241144</td>    <td>0.14566251234607783</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>16.0</td>    <td>0.05</td>     <td>0.05387761875414587</td>  <td>0.005540233103807372</td>   <td>0.04153321012661037</td>   <td>0.06622202738168138</td>  <td>0.02548533512234804</td>   <td>0.0822699023859437</td>   <td>-0.00387761875414587</td>  <td>0.010049154501280098</td>  <td>-0.3858651743937188</td>  <td>0.007542528266574239</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{cccccccccccc}\n",
       "\\toprule\n",
       "\\textbf{Obs} &   \\textbf{Dep Var}  &   \\textbf{Predicted}  &   \\textbf{Std Error}  &    \\textbf{Mean ci}   &   \\textbf{Mean ci}   &  \\textbf{Predict ci}  & \\textbf{Predict ci}  &   \\textbf{Residual}    &  \\textbf{Std Error}  &   \\textbf{Student}   &    \\textbf{Cook's}      \\\\\n",
       " \\textbf{}   & \\textbf{Population} &     \\textbf{Value}    & \\textbf{Mean Predict} &   \\textbf{95\\% low}   &  \\textbf{95\\% upp}   &   \\textbf{95\\% low}   &  \\textbf{95\\% upp}   &       \\textbf{}        &  \\textbf{Residual}   &  \\textbf{Residual}   &       \\textbf{D}        \\\\\n",
       "\\midrule\n",
       "    1.0      &         0.1         &  0.09369968407159773  &  0.008102117681094477 &  0.07564704088315906  & 0.11175232726003641  &  0.06240055834013381  & 0.12499880980306166  & 0.0063003159284022725  & 0.008126215485502302 &  0.7753075142595526  &   0.099590327560833     \\\\\n",
       "    2.0      &         0.03        &  0.031885710092675226 &  0.006921636628078912 &  0.01646334260246901  & 0.04730807758288144  & 0.0020262599224550107 & 0.06174516026289544  & -0.0018857100926752274 & 0.009152629973070993 & -0.20602931596966031 &  0.004046062370333531   \\\\\n",
       "    3.0      &         0.19        &  0.18165420507660063  &  0.008662533046744266 &  0.16235287863871953  & 0.20095553151448173  &   0.1496186225514376  & 0.21368978760176366  &  0.008345794923399374  & 0.007525969057150357 &  1.1089329307659201  &   0.2715340282706297    \\\\\n",
       "    4.0      &         0.12        &   0.1279528395045604  &  0.006135514724779922 &   0.1142820607694753  &  0.1416236182396455  &  0.09895926199697205  & 0.15694641701214876  &  -0.00795283950456041  & 0.009697172170129284 &  -0.820119449777118  &  0.04487606469975594    \\\\\n",
       "    5.0      &         0.04        &  0.03405294374859834  &  0.006463713038543853 &  0.01965089359946643  & 0.048454993897730246 &  0.004707477421361444 & 0.06339841007583523  & 0.0059470562514016626  & 0.00948156647345149  &  0.6272229665903306  &  0.030471694144001856   \\\\\n",
       "    6.0      &        0.095        &  0.09138271560132372  &  0.008794259959050447 &  0.07178788331228382  & 0.11097754789036361  &   0.0591694450765769  & 0.12359598612607053  &  0.003617284398676285  & 0.00737161317540219  & 0.49070458698871267  &  0.05711662119644293    \\\\\n",
       "    7.0      &         0.07        &   0.0572825208009107  &  0.005570586735795068 &  0.044870480066645166 & 0.06969456153517624  &  0.028860766797085678 & 0.08570427480473572  &  0.012717479199089304  & 0.01003236026337298  &  1.2676457847630722  &  0.08257341110055803    \\\\\n",
       "    8.0      &         0.06        &  0.07444992964857097  &  0.004655744136112865 &  0.06407628525408995  & 0.08482357404305199  &  0.04685736102062302  & 0.10204249827651893  & -0.014449929648570975  & 0.010488266566700707 & -1.3777233403323472  &  0.062336664150820884   \\\\\n",
       "    9.0      &         0.02        &  0.04216983066677332  &  0.008192188322617143 &  0.023916497582536587 & 0.06042316375101006  &  0.010754523662878407 & 0.07358513767066824  & -0.022169830666773322  & 0.008035405373837493 &  -2.759018323948678  &   1.3186884176759859    \\\\\n",
       "    10.0     &         0.05        &  0.04016431941021288  & 0.0070397527400107305 &  0.024478772821968343 & 0.05584986599845742  &  0.010168091026683469 &  0.0701605477937423  &  0.009835680589787121  & 0.009062095254113839 &  1.0853649530247549  &  0.11848350762733904    \\\\\n",
       "    11.0     &         0.07        &  0.07118958726881228  &  0.006357304394839466 &  0.05702463035290301  & 0.08535454418472155  &  0.04195975063063628  & 0.10041942390698827  &  -0.00118958726881227  & 0.009553238710849633 & -0.12452188255918416 & 0.0011444184707134649   \\\\\n",
       "    12.0     &         0.1         &  0.09920293411798915  &  0.008063567803063934 &  0.08123618541052884  & 0.11716968282544947  &  0.06795327164650167  & 0.13045259658947664  & 0.0007970658820108528  & 0.008164469567615022 & 0.09762616853548871  & 0.0015494578801624889   \\\\\n",
       "    13.0     &        0.085        &  0.09073574642130422  &  0.006566519363972104 &  0.07610462950425788  & 0.10536686333835056  &   0.0612771835285701  & 0.12019430931403834  & -0.005735746421304214  & 0.009410659513429467 & -0.6094946282052842  &  0.030145247353949547   \\\\\n",
       "    14.0     &         0.09        &  0.08941481364275149  &   0.0062005610211589  &  0.07559910272752794  & 0.10323052455797503  &  0.060352618034747035 & 0.11847700925075594  &  0.00058518635724851   & 0.009655709816373214 & 0.06060521374163585  & 0.00025244239102058935  \\\\\n",
       "    15.0     &         0.01        & 0.0008846011731687984 &  0.007592623790715615 & -0.016032818883277972 & 0.017802021229615567 & -0.029773781804919196 & 0.031542984151256795 &  0.009115398826831201  & 0.008604170675188887 &  1.059416319241144   &  0.14566251234607783    \\\\\n",
       "    16.0     &         0.05        &  0.05387761875414587  &  0.005540233103807372 &  0.04153321012661037  & 0.06622202738168138  &  0.02548533512234804  &  0.0822699023859437  &  -0.00387761875414587  & 0.010049154501280098 & -0.3858651743937188  &  0.007542528266574239   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "st, data, ss2 = summary_table(model, alpha = 0.05)\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c50f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicollinearity\n",
    "\n",
    "Recall that the issue of linearly dependent (or “almost” linearly dependent) column/variables is called multicollinearity.\n",
    "\n",
    "Multicollinearity occurs when a variable is highly or perfectly correlated with a linear combination of one or more columns. \n",
    "- Intuitively, this means that the variable can be deduced from other variables and thus is not capable of adding new information to the model.\n",
    "\n",
    "<b>Structural Multicollinearity</b>: Creating new predictors from other predictors\n",
    "- For example, multicollinearity can occur when $p$ dummy variables are incorporated into a model instead of $p-1$.\n",
    "\n",
    "<b>Data-based multicollinearity</b>: Result of a poorly designed experiment, reliance on purely observational data. Why can't a researcher just collect his data in such a way to ensure that the predictors aren't highly correlated?\n",
    "- Unfortunately, researchers often can't control the predictors. For example when gathering a person's gender, race, grade point average, math SAT score, IQ, and starting salary, one or more variables maybe dependent on others but data collectors in such observational studies cannot control for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba840a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multicollinearity\n",
    "\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Big problem</u> With our design matrix $A$ having dependent columns, $A^TA$ is singular making it non-invertible. This is a violation of a key condition!\n",
    "\n",
    "`statsmodels` should give you a warning when you have issues with multicollinearity. It’s good practice to check your models even if you don’t see this warning!\n",
    "\n",
    "- <b>First check</b>: Derive a correlation matrix of your predictors. Do not include the response variable as we want our predictors to have high correlation with it.\n",
    "- <b>Second check</b>: Variance Inflation Factor to systematically remove variables that can be explained by others.\n",
    "    - $VIF = \\frac{1}{(1-R^2_x)}$; where $R^2_x$ is the variation in $x$ that can be explained by its relationship to other variables in the model.\n",
    "        - Treat x as the response variable that you are trying to predict from the remaining explanatory variables. Calculate R^2 of this auxiliary regression.\n",
    "        - VIF over 10 is worrisome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1bdbe36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>BAC</td>       <th>  R-squared:         </th> <td>   0.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 20 Jan 2026</td> <th>  Prob (F-statistic):</th> <td>2.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:37:51</td>     <th>  Log-Likelihood:    </th> <td>  52.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    16</td>      <th>  AIC:               </th> <td>  -93.08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    10</td>      <th>  BIC:               </th> <td>  -88.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       BAC        & \\textbf{  R-squared:         } &    0.955  \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &    0.932  \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &    42.39  \\\\\n",
       "\\textbf{Date:}             & Tue, 20 Jan 2026 & \\textbf{  Prob (F-statistic):} & 2.06e-06  \\\\\n",
       "\\textbf{Time:}             &     00:37:51     & \\textbf{  Log-Likelihood:    } &   52.538  \\\\\n",
       "\\textbf{No. Observations:} &          16      & \\textbf{  AIC:               } &   -93.08  \\\\\n",
       "\\textbf{Df Residuals:}     &          10      & \\textbf{  BIC:               } &   -88.44  \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &           \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &           \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "First we start with the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e4cbff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#convert boolean column to numeric\n",
    "bac_data['Gender_OSU_male'] = bac_data['Gender_OSU_male'].astype(int)\n",
    "full_model = smf.ols(\n",
    "    'BAC ~ Beers + Weight_OSU + Gender_OSU_male + Sobr1 + Sobr2',\n",
    "    data=bac_data\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70df4a52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>BAC</td>       <th>  R-squared:         </th> <td>   0.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   42.39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 20 Jan 2026</td> <th>  Prob (F-statistic):</th> <td>2.06e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:37:51</td>     <th>  Log-Likelihood:    </th> <td>  52.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    16</td>      <th>  AIC:               </th> <td>  -93.08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    10</td>      <th>  BIC:               </th> <td>  -88.44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       BAC        & \\textbf{  R-squared:         } &    0.955  \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &    0.932  \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &    42.39  \\\\\n",
       "\\textbf{Date:}             & Tue, 20 Jan 2026 & \\textbf{  Prob (F-statistic):} & 2.06e-06  \\\\\n",
       "\\textbf{Time:}             &     00:37:51     & \\textbf{  Log-Likelihood:    } &   52.538  \\\\\n",
       "\\textbf{No. Observations:} &          16      & \\textbf{  AIC:               } &   -93.08  \\\\\n",
       "\\textbf{Df Residuals:}     &          10      & \\textbf{  BIC:               } &   -88.44  \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     } &           \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &           \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dc48070",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>    0.0925</td> <td>    0.159</td> <td>    0.580</td> <td> 0.575</td> <td>   -0.263</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beers</th>           <td>    0.0191</td> <td>    0.002</td> <td>    9.037</td> <td> 0.000</td> <td>    0.014</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight_OSU</th>      <td>   -0.0003</td> <td> 7.45e-05</td> <td>   -4.682</td> <td> 0.001</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gender_OSU_male</th> <td>    0.0013</td> <td>    0.010</td> <td>    0.133</td> <td> 0.897</td> <td>   -0.020</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr1</th>           <td>   -0.0034</td> <td>    0.016</td> <td>   -0.209</td> <td> 0.838</td> <td>   -0.040</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr2</th>           <td>   -0.0023</td> <td>    0.004</td> <td>   -0.639</td> <td> 0.537</td> <td>   -0.010</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       0.0925  &        0.159     &     0.580  &         0.575        &       -0.263    &        0.448     \\\\\n",
       "\\textbf{Beers}             &       0.0191  &        0.002     &     9.037  &         0.000        &        0.014    &        0.024     \\\\\n",
       "\\textbf{Weight\\_OSU}       &      -0.0003  &     7.45e-05     &    -4.682  &         0.001        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{Gender\\_OSU\\_male} &       0.0013  &        0.010     &     0.133  &         0.897        &       -0.020    &        0.022     \\\\\n",
       "\\textbf{Sobr1}             &      -0.0034  &        0.016     &    -0.209  &         0.838        &       -0.040    &        0.033     \\\\\n",
       "\\textbf{Sobr2}             &      -0.0023  &        0.004     &    -0.639  &         0.537        &       -0.010    &        0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4eedb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "While the overall model seems statistically significant, some predictors have high $p$ values and relatively higher standard error. Let's investigate further by computing VIF of all predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "847b6786",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beers</td>\n",
       "      <td>14.565857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weight_OSU</td>\n",
       "      <td>20.175098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gender_OSU_male</td>\n",
       "      <td>4.242286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sobr1</td>\n",
       "      <td>146.555552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sobr2</td>\n",
       "      <td>98.138953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature         VIF\n",
       "0            Beers   14.565857\n",
       "1       Weight_OSU   20.175098\n",
       "2  Gender_OSU_male    4.242286\n",
       "3            Sobr1  146.555552\n",
       "4            Sobr2   98.138953"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "bac_predictors = bac_data[['Beers', 'Weight_OSU', 'Gender_OSU_male', 'Sobr1', 'Sobr2']]\n",
    "\n",
    "vif = pd.DataFrame({\n",
    "    'Feature': bac_predictors.columns,\n",
    "    'VIF': [variance_inflation_factor(bac_predictors.values, i) for i in range(len(bac_predictors.columns))]\n",
    "})\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a509a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "`Sobr1` and `Sobr2` are severely collinear. This is understandable since they are sequential sobriety test results. Remove the feature with the largest VIF and refit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ef2533",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>    0.0599</td> <td>    0.033</td> <td>    1.812</td> <td> 0.097</td> <td>   -0.013</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beers</th>           <td>    0.0190</td> <td>    0.002</td> <td>    9.990</td> <td> 0.000</td> <td>    0.015</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight_OSU</th>      <td>   -0.0003</td> <td>    7e-05</td> <td>   -4.939</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gender_OSU_male</th> <td>    0.0006</td> <td>    0.009</td> <td>    0.066</td> <td> 0.949</td> <td>   -0.018</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr2</th>           <td>   -0.0024</td> <td>    0.003</td> <td>   -0.681</td> <td> 0.510</td> <td>   -0.010</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       0.0599  &        0.033     &     1.812  &         0.097        &       -0.013    &        0.133     \\\\\n",
       "\\textbf{Beers}             &       0.0190  &        0.002     &     9.990  &         0.000        &        0.015    &        0.023     \\\\\n",
       "\\textbf{Weight\\_OSU}       &      -0.0003  &        7e-05     &    -4.939  &         0.000        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{Gender\\_OSU\\_male} &       0.0006  &        0.009     &     0.066  &         0.949        &       -0.018    &        0.019     \\\\\n",
       "\\textbf{Sobr2}             &      -0.0024  &        0.003     &    -0.681  &         0.510        &       -0.010    &        0.005     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = smf.ols(\n",
    "    'BAC ~ Beers + Weight_OSU + Gender_OSU_male + Sobr2',\n",
    "    data=bac_data\n",
    ").fit()\n",
    "\n",
    "model1.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed3036",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "Recompute VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc3266f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beers</td>\n",
       "      <td>5.961561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weight_OSU</td>\n",
       "      <td>18.954708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gender_OSU_male</td>\n",
       "      <td>2.666197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sobr2</td>\n",
       "      <td>11.557791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature        VIF\n",
       "0            Beers   5.961561\n",
       "1       Weight_OSU  18.954708\n",
       "2  Gender_OSU_male   2.666197\n",
       "3            Sobr2  11.557791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bac_predictors = bac_data[['Beers', 'Weight_OSU', 'Gender_OSU_male', 'Sobr2']]\n",
    "\n",
    "vif = pd.DataFrame({\n",
    "    'Feature': bac_predictors.columns,\n",
    "    'VIF': [variance_inflation_factor(bac_predictors.values, i) for i in range(len(bac_predictors.columns))]\n",
    "})\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2830810",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "From the VIF table it seems that `Weight_OSU` and `Sobr2` are highly collinear but we need to be careful before deciding to remove `Weight_OSU`. Looking at the summary table it seems that holding beers consumed, gender, and sobriety score constant, weight is significantly associated with BAC.\n",
    "\n",
    "This seems to indicate that even though Weight is highly predictable from other variables, it still exhibits a robust relationship with BAC.\n",
    "\n",
    "<u><b>Ask yourself</b></u>, does weight add more interpretive value than a sobriety test score, vis-a-vis predicting BAC? If so, <u><b>do not</b></u> blindly remove it!\n",
    "\n",
    "- Instead, let us recenter the measurements, refit the model and recompute VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584c63b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>       <td>    0.0005</td> <td>    0.032</td> <td>    0.017</td> <td> 0.987</td> <td>   -0.069</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beers</th>           <td>    0.0190</td> <td>    0.002</td> <td>    9.990</td> <td> 0.000</td> <td>    0.015</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weight_c</th>        <td>   -0.0003</td> <td>    7e-05</td> <td>   -4.939</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gender_OSU_male</th> <td>    0.0006</td> <td>    0.009</td> <td>    0.066</td> <td> 0.949</td> <td>   -0.018</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sobr2</th>           <td>   -0.0024</td> <td>    0.003</td> <td>   -0.681</td> <td> 0.510</td> <td>   -0.010</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lcccccc}\n",
       "\\toprule\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}         &       0.0005  &        0.032     &     0.017  &         0.987        &       -0.069    &        0.070     \\\\\n",
       "\\textbf{Beers}             &       0.0190  &        0.002     &     9.990  &         0.000        &        0.015    &        0.023     \\\\\n",
       "\\textbf{Weight\\_c}         &      -0.0003  &        7e-05     &    -4.939  &         0.000        &       -0.001    &       -0.000     \\\\\n",
       "\\textbf{Gender\\_OSU\\_male} &       0.0006  &        0.009     &     0.066  &         0.949        &       -0.018    &        0.019     \\\\\n",
       "\\textbf{Sobr2}             &      -0.0024  &        0.003     &    -0.681  &         0.510        &       -0.010    &        0.005     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bac_data['Weight_c'] = bac_data['Weight_OSU'] - bac_data['Weight_OSU'].mean()\n",
    "model2 = smf.ols(\n",
    "    'BAC ~ Beers + Weight_c + Gender_OSU_male + Sobr2',\n",
    "    data=bac_data\n",
    ").fit()\n",
    "\n",
    "model2.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d2036fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beers</td>\n",
       "      <td>3.965409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weight_c</td>\n",
       "      <td>1.458188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gender_OSU_male</td>\n",
       "      <td>3.420349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sobr2</td>\n",
       "      <td>6.396044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature       VIF\n",
       "0            Beers  3.965409\n",
       "1         Weight_c  1.458188\n",
       "2  Gender_OSU_male  3.420349\n",
       "3            Sobr2  6.396044"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bac_predictors = bac_data[['Beers', 'Weight_c', 'Gender_OSU_male', 'Sobr2']]\n",
    "\n",
    "vif = pd.DataFrame({\n",
    "    'Feature': bac_predictors.columns,\n",
    "    'VIF': [variance_inflation_factor(bac_predictors.values, i) for i in range(len(bac_predictors.columns))]\n",
    "})\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee4963",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multicollinearity handling\n",
    "\n",
    "Now all out VIFs are below 10!\n",
    "- Likely explanation is that `Weight_OSU` exhibited strong collinearity with Gender. On average, males could be reasonably assumed to weigh higher than female subjects.\n",
    "- Upon centering, gender is correlated only with deviations from the mean weight rather than the mean itself, so the shared variance shrinks!\n",
    "    - Before centering gender explained mean difference (that was large), after centering it only explains difference in deviation from mean but that's a relatively smaller effect.\n",
    "\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Takeaway</u>: Standardizing variables (especially by centering them, i.e., subtracting the mean) reduces VIF by changing the relationship between predictors. \n",
    "\n",
    "Centering alleviates structural collinearity (often with the intercept and dummy variable), reducing VIF without changing model fit or inference for slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb1e42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assumptions of Linear Regression\n",
    "\n",
    "Some key conditions essential to linear regression:\n",
    "\n",
    "<ul>\n",
    "  <li class=\"fragment\"><u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">L</u><b>inearity</b>: We predict the expected response $E(Y)$ for a given observation $X$ and it exhibits a linear relationship with the explanatory variables (predictors) in $X$</li>\n",
    "  <li class=\"fragment\"><u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">I</u><b>ndependent Residuals</b>: The error terms $\\epsilon_i$ for each prediction (also known as the residual) are independent, meaning that the value of the error for one case gives no information about the value of the error for another case.</li>\n",
    "  <li class=\"fragment\"><u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">N</u><b>ormality of Residuals</b>: The error terms $\\epsilon_i$ for each prediction are n.i.d., i.e. normally distributed around the regression hyperplane (line in 2D).</li>\n",
    "  <li class=\"fragment\"><u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">E</u><b>quality of Variance</b>: The error terms $\\epsilon_i$ have the same variance for all observations</li>\n",
    "</ul>\n",
    "\n",
    "<span class=\"fragment\">An equivalent way to think of linearity is that the conditional mean of the error, \n",
    "$E(\\epsilon_i|X_i)=0$\n",
    ". If the model perfectly captured the average trend, then any deviation (residual) above the line should be cancelled by the ones below it, thereby keeping the average error at 0.</span>\n",
    "\n",
    "<span class=\"fragment\">All four conditions can be summed up by: \n",
    "$\n",
    "P(e|X) \\sim \\mathcal{N(0, \\sigma^2 I_n)}\n",
    "$\n",
    "</span>\n",
    "\n",
    "<span class=\"fragment\">Next we will see how to check if each of these conditions holds.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb857f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linearity\n",
    "\n",
    "- You can check this assumption using scatterplots – one for each predictor vs the response variable.\n",
    "- You can also check for linearity with a scatterplot of the (standardized) residuals, against the predicted values (on the horizontal axis).\n",
    "- What do you do if your data are not linear?\n",
    "    - Transformations (log, square root, inverse, etc.) on either the predictors or responses.\n",
    "\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"images/linearity2.png\" scale=\"0.4;\" style=\"width:40%; margin:3px;\">\n",
    "    <img src=\"images/linearity1.png\" scale=\"0.52;\" style=\"width:55%; margin:3px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050528",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Standardized Residuals\n",
    "\n",
    "- Residuals can be sensitive to the scale of the data and the presence of outliers. In other words, some points are expected to be closer to the line than others.</br> Standardized residuals adjust for that, so we don’t unfairly judge them.\n",
    "- We already know that residuals in a true linear relationship are centered around 0. Standardized residuals $e^{std}_i$ are obtained by dividing the raw residuals by the standard deviation of residuals, se.\n",
    "$\n",
    "e^{std}_i = \\frac{y_i-\\hat{y}_i}{se}\n",
    "$\n",
    "- $P(e^{std}_i|X) \\sim \\mathcal{N(0, 1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00588b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Partial Residuals\n",
    "\n",
    "<span class=\"fragment\">A scatterplot like <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">BAC vs Weight</u> shows the marginal relationship between the response variable and the predictor while ignoring all other predictors. But this is not what we estimate in linear regression.\n",
    "$$\\beta_j = \\text{effect of } X_j \\text{ while holding all other predictors fixed}$$\n",
    "</span>\n",
    "\n",
    "<span class=\"fragment\">A predictor can look weak or nonlinear marginally but show strong conditional linearity or vice versa.\n",
    "</span>\n",
    "\n",
    "<span class=\"fragment\">A partial residual with respect to the $j$-th predictor is\n",
    "$$\\begin{aligned}\n",
    "r^{(j)}_i &= y_i - \\sum\\limits_{k \\neq j} \\hat{\\beta}_kx_{ik} \\implies\n",
    "r^{(j)}_i = \\hat{\\beta}_jx_{ij} + e_i\n",
    "\\end{aligned}$$\n",
    "</span>\n",
    "\n",
    "<span class=\"fragment\">Sometimes this is thought of as an added value–how much else does this particular predictor contribute? Marginal plots cannot show this.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ca125",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Independence and Uncorrelated Residuals\n",
    "\n",
    "We are assuming that each observation and corresponding residual is independent from all of others.<br> \n",
    "What we want to check is for the residuals to be uncorrelated, no general pattern among the residuals across observations or predicted values.\n",
    "- Scatterplots of residuals against predictors and response variables.\n",
    "\n",
    "Most commonly violated in\n",
    "- Time-series data\n",
    "- Spatial data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0a138",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Normality of Residuals\n",
    "\n",
    "We can simply plot a histogram of residuals to check normality.<br>\n",
    "<b>Possible remedies</b>: Transformations (log, square root, inverse, etc.) on either the predictors or responses.\n",
    "<div style=\"display: flex; align-items: center; gap: 5px;\">\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <p>What <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">does not</u> break: Least squares coefficient estimates and their standard errors, linearity of the fitted model,     $R^2$, prediction accuracy (subject to presence of outliers).<br>\n",
    "    Although the tests and and confidence intervals originate from the normal distributions, the consequences are usually minor, because the formulas are based on the sampling distributions of the estimates.\n",
    "    </p>\n",
    "    <p>What <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">does</u> break: Hypothesis Tests, Confidence Intervals for means and Prediction Intervals all hold strong assumptions of normality. t-statistics etc. are no longer valid. Outlier diagnostics (more on this later!)</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <img src=\"images/normality.png\" alt=\"Confidence and Prediction Intervals\" scale=\"0.45;\" style=\"width: 90%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ca7f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Equality of Variance\n",
    "\n",
    "We can plot the residuals against the predictors.\n",
    "- What we want is <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Homoscedasticity</u>: equal spread of residuals along either side of the mean response for all observations.\n",
    "\n",
    "Violation of this condition is called <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Heteroscedasticity</u> and it has far more serious consequences.\n",
    "- Standard errors inaccurately describe the uncertainty in the estimates, as a consequence means tests, confidence intervals and p-values can be misleading.\n",
    "- Could rebuild the model with different independent variables.\n",
    "- Perform transformations on non linear data or fit other (non linear) models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f33fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Homoscedasticity\n",
    "<img src=\"images/homoscedasticity.png\" alt=\"Confidence and Prediction Intervals\" scale=\"0.5;\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076809e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Heteroscedasticity\n",
    "\n",
    "<img src=\"images/heteroscedasticity.png\" alt=\"Confidence and Prediction Intervals\" scale=\"0.5;\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57994f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Non Linearity\n",
    "\n",
    "<img src=\"images/non-linearity.png\" alt=\"Confidence and Prediction Intervals\" scale=\"0.5;\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac956d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outliers and Influential Observations\n",
    "\n",
    "Always do scatter-plots. They are conceptually simple but can reveal the shape and pattern of your data and reveal a lot of information.\n",
    "\n",
    "<span class=\"fragment\">\n",
    "    <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Outliers</u>: Can influence the regression line/hyperplane by pulling it in the direction of the outlier.\n",
    "    <ul>\n",
    "    <li>A value that can appear out of the norm or extreme for at least one of the variables; does not follow the predicted pattern of the model.</li>\n",
    "    <li>A point whose value may fall within the expected range but produces an extremely large residual.</li>\n",
    "    </ul>\n",
    "    Removing an observation simply because it is an outlier is not a good idea, but it’s also not a good idea to report a conclusion that rests on one or two data points.<br><br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b9cf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Outliers and Influential Observations\n",
    "\n",
    "<span class=\"fragment\">\n",
    "    <u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Influential Observation</u>: It could be an outlier, but not necessarily. The point has a disproportionate effect on model estimates and interpretations.\n",
    "    <ul>\n",
    "    <li><u><b>High Leverage Point</b></u>: Could be a value corresponding to an observation that <u>horizontally</u> further away from the rest of the observations in the sample. These points exhibit very strong influence on the least squares plane.<br>Some outliers are high leverage points, but all high leverage points are <b>NOT</b> outliers.</li>\n",
    "    <li>It could be a value of the dependent variable that is far outside of the rest of the values for the data.</li>\n",
    "    </ul>\n",
    "</span>\n",
    "\n",
    "<span class=\"fragment\">\n",
    "Outliers are unusual in the response. Influential observations are unusual in how much they affect the fitted model.<br>\n",
    "Influential observations can dramatically change the regression outputs, model coefficients, strength and direction of relationship between predictors and the response variable, as well as model significance and standard error.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edca78",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Influential Observations\n",
    "\n",
    "<div style=\"display: flex; align-items: center; gap: 5px;\">\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <p>Influential Points are ones that actually <b>invoke influence on the slope of the line</b>. They are often (but not always) high leverage points.</br> Influence is a combination of leverage and how extreme a point is in the $y$-direction.\n",
    "    <ul>\n",
    "        <li>Influential points could change a strong relationship to a weak relationship.</li>\n",
    "        <li>Influential points could change a weak relationship to a strong relationship.</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p>High leverage outlier points are very influential!</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <img src=\"images/influential-points-lev-outlier.png\" alt=\"Influential Points\" scale=\"0.45;\" style=\"width: 90%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d299ab1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to deal with influential observations?\n",
    "\n",
    "- One way to assess the influence of an observation is to temporarily remove the observation to see whether the answers to the questions of interest change.\n",
    "    - Does the evidence from a test change from slight to convincing? Does the decision to include a term in the model change?\n",
    "    - Does an important estimate change by a practically relevant amount?\n",
    "- If not, the observation is not influential.\n",
    "- If so, we need to deal with it!\n",
    "- If an influential observation is not particularly unusual in its explanatory variables, and if no definitive explanation for its unique behavior can be found,\n",
    "omitting it cannot be justified.\n",
    "    - More data are needed to answer the questions of interest.\n",
    "    - As a last resort, report sensitivity: the results with and without the influential observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c146bd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case-Influence Statistics\n",
    "\n",
    "Case-influence statistics are numerical measures that reveal insight regarding individual influence of each observation (or “case”) on the overall regression.\n",
    "- They can help identify influential observations that may not be visible in a graph\n",
    "- They partition the overall influence of an observation into what is unusual about its explanatory variable values and what is unusual about its response relative to the fitted model.\n",
    "\n",
    "We will discuss three different types of case-influence statistics: \n",
    "1. Leverage\n",
    "2. Studentized residuals\n",
    "3. Cook’s distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee04198",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Leverage\n",
    "\n",
    "The leverage of a case is a measure of the distance between its explanatory values and the average of the explanatory variable values in the entire dataset. \n",
    "\n",
    "- For simple linear regression with one predictor, leverage is given as the proportion of the total sum of squares in $X$ due to a single case, but it can also be re-expressed as the distance from the center of the dataset in terms of standard deviations.\n",
    "$$\n",
    "h_i = \\frac{(X_i-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2} + \\frac{1}{n}\n",
    "$$\n",
    "\n",
    "- For multiple explanatory variables, the calculation is more involved (not included) but the concept is the same: <i>proportion of the total sum of squares due to a single case.</i>\n",
    "\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Rule of thumb</u>: Although model dependent, if $h_i>2\\frac{p}{n}$ the case should be flagged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef44101",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Estimating Leverage in Practice\n",
    "\n",
    "Let us check the leverage of individual observations on the reduced model we ended up with after handling multicollinearity! This model uses 4 predictors. As we will see, there are some high leverage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24630ee4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leverage of all 16 points: [0.22596846 0.3596798  0.56095405 0.27875375 0.20131429 0.54924536\n",
      " 0.22692039 0.14906195 0.50928662 0.37632013 0.27584885 0.20429398\n",
      " 0.31281635 0.24972917 0.37267669 0.14713016]\n",
      "5.000000000000002\n",
      "[False False False False False False False False False False False False\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "influence = model2.get_influence()\n",
    "leverage = influence.hat_matrix_diag\n",
    "print(\"Leverage of all 16 points:\", leverage)\n",
    "print(leverage.sum())\n",
    "#checking individually for high leverage as per rule of thumb\n",
    "print(leverage>2*5/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed13e5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bac_data.plot.scatter( x='Weight_c', y='BAC',title='Scatter Plot of BAC vs Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e1165",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cook’s distance.\n",
    "Cook's distance measure the effect that omitting a case has on the estimated regression coeﬀicients. Joint effect of leverage and residual. \n",
    "$$D_i = \\sum\\limits_{j=1}^{n}\\frac{\\hat{y_{j(i)}}-\\hat{y_j}}{p\\times se^2}$$\n",
    "where $\\hat{y_j}$ is the $j$-th fitted value in a fit using all points, $\\hat{y_{j(i)}}$ is the $j$-th fitted value in a fit that excludes the $i$th point in the dataset, $p$ is the number of regression coefficients, and $se^2$ is our estimated variance from the fit based on all observations (the square of our standard error as seen many times before).\n",
    "\n",
    "A case that is influential in terms of changing the least squares estimates when deleted will have a high Cook's Distance.\n",
    "\n",
    "<u style=\"color: blue; font-size: 1.0em; font-weight: bold;\">Rule of thumb</u>: $\\approx$ 1 or above indicate large influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d9c4e70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Frame with Cook's Distance:\n",
      " 0     0.015976\n",
      "1     0.004372\n",
      "2     0.355739\n",
      "3     0.059289\n",
      "4     0.013810\n",
      "5     0.075076\n",
      "6     0.105788\n",
      "7     0.068567\n",
      "8     1.721925\n",
      "9     0.156125\n",
      "10    0.000514\n",
      "11    0.002343\n",
      "12    0.040019\n",
      "13    0.000006\n",
      "14    0.149085\n",
      "15    0.007064\n",
      "Name: cooks_d, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get Cook's distance\n",
    "sm_fr = influence.summary_frame()\n",
    "print(\"Summary Frame with Cook's Distance:\\n\", sm_fr['cooks_d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f86b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Leverage vs Cook's Distance\n",
    "\n",
    "It is hard for an observation with low leverage to have high Cook's Distance.\n",
    "\n",
    "<img src=\"images/leverage-vs-cook.png\" alt=\"Leverage and Influence\" scale=\"0.5;\" style=\"width: 70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff474a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Studentized residuals\n",
    "\n",
    "A high leverage point is one where the observation is further away from the general trend of predictor values, i.e. a sparse region of the predictor(feature) space. Since the regression model is forced to pass close to the point, its residual is naturally constrained to have smaller variance. In such a case, the residual plot will not direct any attention even if the corresponding observation is influential or problematic, relative to its natural scale.\n",
    "\n",
    "A high leverage point can have a small residual and yet be highly influential. This is why <b>influence $\\neq$ outlier</b> and leverage must be accounted for. On the contrary, large residuals at low-leverage points can look alarming but often don’t matter.\n",
    "\n",
    "Raw residuals can be misleading. Hence we standardize them while also rescaling them to their natural variability.\n",
    "- What is a large residual and what is not is unclear without standardization.\n",
    "\n",
    "<b><u style=\"color: blue;\">Studentized Residual</u></b>: This ensures that our residuals approximately follow a standard normal distribution. 95\\% of the residuals must fall between -2 and 2 and those beyond that can be considered unusual.\n",
    "- Internally studentized residual: $\\frac{e_i}{se\\sqrt{1-h_i}}$; $h_i$ is the leverage of the corresponding observation\n",
    "- Internally studentized residual: $\\frac{e_i}{se_{(i)}\\sqrt{1-h_i}}$; $se_{(i)}$ is the estimated standard deviation of fit <u>after removing</u> the observation in question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b12a5157",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52308864, -0.1972758 ,  1.17989119, -0.87579533,  0.52339976,\n",
       "        0.55503688,  1.34238699, -1.39896462, -2.88021482,  1.13742758,\n",
       "       -0.0821076 ,  0.21362983, -0.66299171,  0.00962923,  1.12016422,\n",
       "       -0.4524762 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studres = influence.get_resid_studentized_external()\n",
    "studres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9c131",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thats all folks!\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/endslide.png\" alt=\"Linear Regression The End\" scale=\"0.01;\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
