{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295f0215",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 2px;\">\n",
    "  \n",
    "  <div style=\"text-align: left; padding: 0;\">\n",
    "   <h2 style=\"font-size: 1.8em; margin-bottom: 0;\"><b>Non Frequentist's guide to Statistical Inference...</b></h2>\n",
    "   <br>\n",
    "   <h3 style=\" font-size: 1.2em;margin-bottom: 0;\">Bayesian Analysis</h3>\n",
    "   <h3 style=\"font-size: 1.2em; margin-bottom: 0; color: blue;\"><i>Dr. Satadisha Saha Bhowmick</i></h3>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"margin-right: 5px; padding: 0;\">\n",
    "    <img src=\"images/intro-pic.png\" align=\"right\" alt=\"intro-pic\" style=\"width: 70%;\">\n",
    "    <!-- TEXT NEXT TO IMAGE -->\n",
    "      <div style=\"font-size: 0.5em;\">\n",
    "        <p>Woman teaching geometry, from a fourteenth-century edition of Euclid‚Äôs geometry book.</p>\n",
    "      </div>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a95a6c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "from ipywidgets import interact\n",
    "import sklearn.metrics as metrics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a18e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Module 3: Learning Outcomes\n",
    "Statistical Inference from a Bayesian's POV\n",
    "\n",
    "<div style=\"display: flex; gap: 2px;\">\n",
    "\n",
    "  <div style=\"flex: 1;\">\n",
    "  <ul>\n",
    "    <li class=\"fragment\">Understand the basic idea behind Bayesian inference.</li>\n",
    "    <li class=\"fragment\">Derive a posterior distribution, given a prior one and a likelihood function.</li>\n",
    "    <li class=\"fragment\">Use conjugate pairs to derive posterior distributions.</li>\n",
    "  </ul>\n",
    "\n",
    "  \n",
    "  </div>\n",
    "<!--\n",
    "  <div style=\"flex: 1;\">\n",
    "  <ul>\n",
    "    <li class=\"fragment\">Posterior Simulation and Analysis.</li>\n",
    "    <li class=\"fragment\">Use Markov chains to model certain types of problems.</li>\n",
    "    <li class=\"fragment\">Find the steady states for certain types of problems.</li>\n",
    "  </ul>\n",
    "  </div>\n",
    "!-->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430723e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Bayesian World View\n",
    "\n",
    "### Squared and Polynomial Terms\n",
    "<p>As children, it takes a few spills to understand that liquid doesn‚Äôt stay in a glass.<br> Or a couple attempts at conversation to understand that, unlike in cartoons, real dogs can‚Äôt talk.</p>\n",
    "<div style=\"display: flex; align-items: center; gap: 5px;\">\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <p>Bayesian Knowledge Building...</p> \n",
    "    <p>Repeating an iterative process:\n",
    "    <ul>\n",
    "    <li>Acknowledge your <b>prior</b> beliefs. We do not walk into inquiries without context.</li>\n",
    "    <li>Gather <b>data</b> about the world.</li>\n",
    "    <li>Use data to update knowledge <i><b>a posteriori</b></i>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p>Apply this Bayesian process to all rigorous research inquiries.</p>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"; style=\"flex: 1;\">\n",
    "    <img src=\"images/bayes_diagram.png\" alt=\"Bayesian Knowledge Building\" scale=\"0.75;\" style=\"width: 90%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387de472",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian vs Frequentist\n",
    "\n",
    "Statistical modeling is broadly divided into these two camps.\n",
    "- <b>REMINDER!</b> They both believe in Bayes' Theorem!\n",
    "- Frequentist philosophy has traditionally dominated statistics.\n",
    "\n",
    "Commonalities:\n",
    "- Common goal: learn from data about the world around us.\n",
    "    - Both Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\n",
    "- Similar inferences: when working with the same data, they will typically produce a similar set of conclusions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dae81",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian vs Frequentist: Key Difference\n",
    "\n",
    "- Meaning of Probability\n",
    "    - Bayesians believe a probability measures the relative plausibility of an event.\n",
    "    - Frequentists interpret probability as the long-run relative frequency of a repeatable event.\n",
    "    - For one-time events, the frequentist interpretation often cracks. \n",
    "        - If a pollster says that candidate $A$ has a 0.9 probability of winning the upcoming election, a Bayesian would interpret that, based on election models, the relative plausibility of winning is high, the candidate is 9 times more likely to win than to lose. \n",
    "        - For frequentists, the long-run relative frequency concept of observing the election over and over simply doesn‚Äôt apply here. They may say that the pollster is simply wrong! Or in a more flexible intepretation state that in long-run <i>hypothetical</i> repetitions of the election candidate $A$ would win roughly $90\\%$ of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e7607",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian vs Frequentist: Key Difference\n",
    "\n",
    "- Treatment of Parameters and Data\n",
    "    - Bayesians treat parameters as random variables with a *prior* distribution gathered from historical data. Observed data is fixed and is used to update the distribution of beliefs about the parameters.\n",
    "    - Frequentists rely solely on current experimental data. For them parameters are *fixed constants* that are simply unknown. The data collected is considered a random sample from a fixed population.\n",
    "\n",
    "**Freuqentists require larger samples for reliable estimates since their starting point of analysis is with the observed data. Bayesian analysis if more robust with small sample sizes due to prior info.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9b08d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Big Idea\n",
    "\n",
    "In classic probability, we build a model that estimates the probability of $\\text{Data}$ given out model's parameters $\\Theta$.\n",
    "$$\n",
    "P(\\text{Data}|\\Theta)\n",
    "$$\n",
    "For example, if our probability model suggests that the data should follow a Gaussian distribution $\\mathcal{N(\\mu, \\sigma^2)}$, then parameters $\\mu$ and $\\sigma$ specify the likelihood of our observed data points.\n",
    "\n",
    "Bayesian Inference flips this idea on it's head. We have measurements of our observed data that comes from a system (*a probabilistic process*). What we want is to derive an inferred model, *a specific set of model parameters*, that is most likely given our observations. $\\rightarrow$ **A statistical inverse problem**\n",
    "\n",
    "Applying Bayes Theorem,\n",
    "$$\n",
    "P(\\Theta|\\text{Data}) = \\frac{P(\\text{Data}|\\Theta)\\times P(\\Theta)}{P(\\text{Data})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ffc06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some Important Notations\n",
    "\n",
    "- $\\displaystyle \\Theta$: The parameter of interest  \n",
    "- $\\displaystyle P(\\Theta)$: The **prior** distribution of the parameter of interest  \n",
    "- $\\displaystyle P(\\Theta \\mid \\text{Data})$: The updated, or **posterior**, distribution of the parameter  \n",
    "- $\\displaystyle P(\\text{Data} \\mid \\Theta)$: The **likelihood** of the data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742400d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prior Distribution\n",
    "\n",
    "- The **prior distribution**, $\\displaystyle P(\\Theta)$, is the distribution describing beliefs about an unknown quantity (a.k.a., our parameter) before new evidence is taken into account.\n",
    "\n",
    "- Think of this as your loose preconceived notions about what you are studying.\n",
    "\n",
    "- The prior distribution is something **YOU** get to pick!\n",
    "    - If you have no evidence, you might use something called a *naive prior*.\n",
    "    - If you have some evidence (maybe from a pilot study or previous knowledge), you‚Äôd use more of an ‚Äúeducated guess.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e92d8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "- The **likelihood**, $P(\\text{Data} \\mid \\Theta)$, provides a framework to compare the relative compatibility of our data with a particular value of $\\Theta$.\n",
    "\n",
    "- It is **NOT** a probability! The normal rules for probabilities do not apply.\n",
    "\n",
    "- It is, however, something closely related to probability, and you get to ‚Äúpick‚Äù it (although sometimes picking is just using the most common option in a particular scenario ‚Äî more on that later).\n",
    "\n",
    "- Formally, with $n$ observations $d_i$:\n",
    "\n",
    "$$\n",
    "\\ell(\\Theta)\n",
    "=\n",
    "P(\\text{Data} \\mid \\Theta)\n",
    "=\n",
    "\\prod_{i=1}^{n} P(d_i \\mid \\Theta)\n",
    "$$\n",
    "\n",
    "- Occasionally (mostly), you will see **log-likelihood**, as it‚Äôs easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74cd58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Posterior Distribution\n",
    "\n",
    "- The **posterior distribution**, $P(\\Theta \\mid \\text{Data})$, contains information on our beliefs *updated by the data* from which we can estimate $\\Theta$.\n",
    "\n",
    "- The posterior distribution is something you **derive**, using\n",
    "\n",
    "$$\n",
    "P(\\Theta \\mid \\text{Data})\n",
    "\\propto\n",
    "P(\\text{Data} \\mid \\Theta)\\, P(\\Theta)\n",
    "$$\n",
    "\n",
    "- Our statistical inverse!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab673a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probability of Data\n",
    "\n",
    "You may have noticed that the formula for the posterior does not include $P(\\text{Data})$ even though we did see it earlier. Why?\n",
    "$$\n",
    "P(\\Theta \\mid \\text{Data})\n",
    "\\propto\n",
    "P(\\text{Data} \\mid \\Theta)\\, P(\\Theta)\n",
    "$$\n",
    "\n",
    "- Long story short: we don‚Äôt know, and we don‚Äôt care.\n",
    "    - $P(\\text{Data})$ in this context is operationally a  **normalizing constant**.\n",
    "    - It‚Äôs really hard to find the probability of the data ‚Äî you can make assumptions, but you don‚Äôt really have any way of saying this exact dataset has this exact probability.\n",
    "    - The posterior distribution **IS A DISTRIBUTION!!** We know the rules for distributions, including that they should integrate to one. We can just figure out a normalizing constant later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79648937",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Update\n",
    "\n",
    "Bayesian Statistics is all about applying some prior assumptions about our probability distributions and then updating that prior knowledge *after* observing new evidence.\n",
    "\n",
    "Posterior is nothing but an updated prior.\n",
    "- Can get a batch of new data to update priors.\n",
    "- Or sequentially, one at a time, like a coin flip.\n",
    "\n",
    "Bayesian Inference *balances* evidence with prior beliefs.\n",
    "- Upon 3 successive coin flips, each yielding *Heads*, MLE (the frequentist approach) would state that $P(\\Theta)= P(Heads) = 1.0$. That is absurd, this could just be a bad luck of draw. This is a big problem esp. when working with small samples.\n",
    "- Starting with a reasonable prior and then gradually updating it with evidence from observed data prevents such reckless conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a60b62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bayesian Update\n",
    "\n",
    "- Sequential order of data collection.\n",
    "- Iteratively updating priors at each time step with posterior computed at the previous time step.\n",
    "\n",
    "$$\n",
    "P_k(\\Theta \\mid \\text{Data}) = \\frac{P_k(\\text{Data} \\mid \\Theta)\\times P_k(\\Theta)}{P_k(\\text{Data})}\n",
    "$$\n",
    "$$\n",
    "P_{k+1}(\\Theta) = P_k(\\Theta \\mid \\text{Data}) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de64fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Update\n",
    "\n",
    "**Question:** During a coin flip experiment, can we use Bayesian hypothesis testing to check if the coin fair or biased? The notion of an unfair or biased coin is one where the probability of obtaining heads on a flip is $0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06adf3fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Robust GIF export: Bayesian posterior update after each flip\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "\n",
    "np.random.seed(42)\n",
    "gif_path = \"images/bayesian_posterior_update.gif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d55e77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "# True coin (change to 0.5 or 0.7 to experiment)\n",
    "true_p = 0.7\n",
    "n_flips = 60\n",
    "\n",
    "# Hypotheses\n",
    "p_fair = 0.5\n",
    "p_biased = 0.7\n",
    "\n",
    "# Prior probabilities\n",
    "prior_fair = 0.5\n",
    "prior_biased = 0.5\n",
    "\n",
    "# Store posterior over time\n",
    "posterior_biased_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2d278",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Sequential Bayesian updates\n",
    "# -----------------------------\n",
    "\n",
    "# Simulate coin flips (1 = heads, 0 = tails)\n",
    "flips = np.random.binomial(1, true_p, n_flips)\n",
    "\n",
    "for i in range(n_flips):\n",
    "    flip = flips[i]\n",
    "\n",
    "    # Likelihoods for this flip\n",
    "    if flip == 1:\n",
    "        likelihood_fair = p_fair\n",
    "        likelihood_biased = p_biased\n",
    "    else:\n",
    "        likelihood_fair = 1 - p_fair\n",
    "        likelihood_biased = 1 - p_biased\n",
    "\n",
    "    # Bayes update\n",
    "    numerator = likelihood_biased * prior_biased\n",
    "    denominator = (\n",
    "        likelihood_fair * prior_fair +\n",
    "        likelihood_biased * prior_biased\n",
    "    )\n",
    "\n",
    "    posterior_biased = numerator / denominator\n",
    "    posterior_fair = 1 - posterior_biased\n",
    "\n",
    "    # Update priors for next step\n",
    "    prior_biased = posterior_biased\n",
    "    prior_fair = posterior_fair\n",
    "\n",
    "    posterior_biased_history.append(posterior_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "236c0aa3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Robust GIF export: Bayesian posterior update after each flip\n",
    "# -----------------------------\n",
    "# Animate and save GIF\n",
    "# -----------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(1, n_flips)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Flip number\")\n",
    "ax.set_ylabel(\"Posterior P(biased)\")\n",
    "ax.axhline(0.5, linestyle=\"--\")\n",
    "line, = ax.plot([], [])\n",
    "dot,  = ax.plot([], [], marker=\"o\", linestyle=\"\")\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    dot.set_data([], [])\n",
    "    ax.set_title(\"Bayesian Updating\")\n",
    "    return line, dot\n",
    "\n",
    "def update(i):\n",
    "    # i = 0..n_flips-1\n",
    "    x = np.arange(1, i + 2)\n",
    "    y = posterior_biased_history[: i + 1]\n",
    "    line.set_data(x, y)\n",
    "    dot.set_data([i + 1], [y[-1]])\n",
    "    ax.set_title(f\"Flip {i+1}: {'H' if flips[i]==1 else 'T'}\")\n",
    "    return line, dot\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=n_flips, init_func=init, blit=False)\n",
    "\n",
    "# Important: save *before* closing the figure, keep ani referenced\n",
    "writer = PillowWriter(fps=fps)\n",
    "ani.save(gif_path, writer=writer)\n",
    "\n",
    "plt.close(fig)\n",
    "# print(\"Saved:\", gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe4dc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bayesian_posterior_update.gif\" alt=\"Bayesian Knowledge Building\" scale=\"0.75;\" style=\"width: 90%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43230e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conjugate Priors\n",
    "\n",
    "- Under carefully chosen settings, the posterior distribution belongs to the same family of distributions as the prior distribution.\n",
    "\n",
    "- If so, the prior is called a **conjugate prior**. A conjugate prior must have a ‚Äúmatching‚Äù likelihood pair.\n",
    "\n",
    "- Conjugate priors are particularly convenient because they reduce Bayesian updating to modifying the parameters of a distribution.\n",
    "    - The math behind updating the parameters is often **MUCH** simpler than doing a full derivation, which usually involves integrals.\n",
    "\n",
    "- Check out <a href=\"https://en.wikipedia.org/wiki/Conjugate_prior\" target=\"_blank\">Wikipedia‚Äôs Table of Conjugate Prior Pairs!!</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f691ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some of the most popular conjugate pairs:\n",
    "\n",
    "| Parameter | Data | Prior | Likelihood | Posterior |\n",
    "|------------|------|--------|------------|------------|\n",
    "| $p$ | $d_i \\; (N)$ | $\\text{Beta}(p \\mid \\alpha, \\beta)$ <br> $\\sim p^{\\alpha}(1-p)^{\\beta}$ | $\\text{Binomial}(p \\mid d_i, N)$ <br> $\\sim p^{d_i}(1-p)^{N-d_i}$ | $\\text{Beta}(p \\mid \\alpha + d_i, \\beta + N - d_i)$ <br> $\\sim p^{\\alpha+d_i}(1-p)^{\\beta+N-d_i}$ |\n",
    "| $\\lambda$ | $d_i$ | $\\text{Gamma}(\\lambda \\mid \\alpha, \\beta)$ <br> $\\sim \\lambda^{\\alpha-1} e^{-\\beta \\lambda}$ | $\\text{Poisson}(\\lambda \\mid d_i)$ <br> $\\sim \\lambda^{d_i} e^{-\\lambda}$ | $\\text{Gamma}(\\lambda \\mid \\alpha + d_i, \\beta + 1)$ <br> $\\sim \\lambda^{\\alpha-1+d_i} e^{-(\\beta+1)\\lambda}$ |\n",
    "| $x$ | $d_i$ | $\\text{Normal}(x \\mid \\mu, \\sigma)$ <br> $\\sim e^{-(x-\\mu)^2 / 2\\sigma^2}$ | $\\text{Normal}(x \\mid d_i, s)$* <br> $\\sim e^{-(x-d_i)^2 / 2s^2}$ | $\\text{Normal}\\!\\left(x \\mid \\frac{s^2\\mu + \\sigma^2 d_i}{\\sigma^2 + s^2}, \\frac{\\sigma^2 s^2}{\\sigma^2 + s^2}\\right)$ <br> $\\sim e^{-\\left(x - \\frac{s^2\\mu + \\sigma^2 d_i}{\\sigma^2 + s^2}\\right)^2 \\big/ \\left(\\frac{2\\sigma^2 s^2}{\\sigma^2 + s^2}\\right)}$ |\n",
    "\n",
    "---\n",
    "\n",
    "\\* If the value of $s$ is unknown, the population variance needs to be estimated from multiple data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde1db5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some Fundamental Bayesian Models\n",
    "\n",
    "- The **Beta-Binomial model**: probability that it rains tomorrow in Australia using data on binary categorical variable $Y$, whether or not it rains for each of 1000 sampled days.\n",
    "- The **Gamma-Poisson model**: the rate of bald eagle sightings in Ontario, Canada using data on variable $Y$, the **counts** of eagles seen in each of 37 one-week observation periods.\n",
    "- The **Normal-Normal model**: the average 3 p.m. temperature in Australia using data on the bell-shaped variable $Y$, temperatures on a sample of study days.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515a31b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ü™ô The Beta‚ÄìBinomial Bayesian Model  \n",
    "\n",
    "1Ô∏è‚É£ The Setup: Example: Estimating a Gambler‚Äôs Success Probability $p$, as successive bets are placed.<br>\n",
    "Let $p = \\text{probability of success (win)}$.<br>\n",
    "Assume independent bets. Individual outcomes follow\n",
    "$Y_i \\mid p \\sim \\text{Bernoulli}(p)$<br>\n",
    "For $n$ bets:\n",
    "$\n",
    "Y = \\sum_{i=1}^n Y_i \\sim \\text{Binomial}(n, p)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c97d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model  \n",
    "\n",
    "2Ô∏è‚É£ Likelihood \n",
    "\n",
    "- Suppose we observe $$y = \\text{number of wins out of } n \\text{ bets}$$\n",
    "- Likelihood: $P(y \\mid p) = \\binom{n}{y} p^y (1-p)^{n-y}$\n",
    "- Ignoring constants in $p$: $L(p) \\propto p^y (1-p)^{n-y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2585b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model  \n",
    "\n",
    "3Ô∏è‚É£ Choosing a Prior\n",
    "\n",
    "- Since $p \\in (0,1)$, a natural prior is $p \\sim \\text{Beta}(\\alpha, \\beta)$.\n",
    "    - A Beta distribution is a type of continuous probability distribution defined on the interval $[0,1]$.\n",
    "    - It helps us model probabilities. While a Binomial distribution counts \"how many successes,\" the Beta distribution models \"how likely is success?\"\n",
    "- PDF: $\\pi(p) = \\frac{1}{B(\\alpha,\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}$\n",
    "- Interpretation:\n",
    "    - $\\alpha - 1$ prior \"successes\"\n",
    "    - $\\beta - 1$ prior \"failures\"\n",
    "- Prior mean: $\\mathbb{E}[p] = \\frac{\\alpha}{\\alpha+\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4363658",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model  \n",
    "\n",
    "4Ô∏è‚É£ Posterior Derivation\n",
    "\n",
    "We learnt earlier that the posterior probability distribution is given by: $\\pi(p \\mid y) \\propto L(p)\\pi(p)$\n",
    "\n",
    "Substituting our prior and likelihood distributions into this, we get:\n",
    "$$\n",
    "\\pi(p \\mid y) \\propto p^y (1-p)^{n-y}\\times p^{\\alpha-1}(1-p)^{\\beta-1}\n",
    "$$\n",
    "\n",
    "Upon combining exponents, we can easily recognize the kernel of an *updated* Beta distribution, thereby establishing conjugacy.\n",
    "$$\n",
    "\\pi(p \\mid y) \\propto p^{\\alpha+y-1}(1-p)^{\\beta+n-y-1}\n",
    "$$\n",
    "$$\n",
    "\\boxed{\n",
    "p \\mid y \\sim \\text{Beta}(\\alpha+y,\\ \\beta+n-y)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987db1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model\n",
    "\n",
    "5Ô∏è‚É£ Interpretation: Hypothetical Counts (recall this is the allowance 'relaxed frequentists' make too!).<br>\n",
    "Posterior parameters: $\\alpha_{\\text{post}} = \\alpha + y$; $\\beta_{\\text{post}} = \\beta + n - y$<br>\n",
    "So Bayesian updating becomes : Prior pseudo-counts + Observed counts\n",
    "\n",
    "6Ô∏è‚É£ Posterior expectation: $\\mathbb{E}[p \\mid y] = \\frac{\\alpha+y}{\\alpha+\\beta+n}$\n",
    "\n",
    "With a bit of algebraic manipulation:\n",
    "$$\n",
    "=\n",
    "\\frac{\\alpha+\\beta}{\\alpha+\\beta+n}\n",
    "\\cdot\n",
    "\\frac{\\alpha}{\\alpha+\\beta}\n",
    "+\n",
    "\\frac{n}{\\alpha+\\beta+n}\n",
    "\\cdot\n",
    "\\frac{y}{n}\n",
    "$$\n",
    "\n",
    "> Posterior mean = weighted average of prior mean and sample proportion\n",
    "\n",
    "As $n \\to 0$: Posterior ‚âà Prior.<br>\n",
    "As $n \\to \\infty$: $\\mathbb{E}[p \\mid y] \\to \\frac{y}{n}$. Meaning that the data dominates and bayesian learning gradually shifts belief toward evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54b19f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model\n",
    "\n",
    "8Ô∏è‚É£ Prior Choices in the Gambler Example\n",
    "\n",
    "- Naive prior: $\\text{Beta}(1,1)$\n",
    "    - Uniform on $(0,1)$\n",
    "- Skeptical prior (centered at fairness): $\\text{Beta}(5,5)$\n",
    "    - Expected probability of success at $0.5$.\n",
    "- Strong belief gambler is skilled: $\\text{Beta}(14,6)$\n",
    "    - Expected probability of success at $0.7$.\n",
    "Different priors ‚Üí different posterior behavior for small $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648066f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ü™ô The Beta‚ÄìBinomial Bayesian Model\n",
    "\n",
    "9Ô∏è‚É£ Posterior Predictive Distribution\n",
    "\n",
    "Probability next bet is a win:$P(Y_{\\text{new}} = 1 \\mid y)=\\mathbb{E}[p \\mid y]=\\frac{\\alpha+y}{\\alpha+\\beta+n}$\n",
    "\n",
    "The marginal distribution of wins: $Y \\sim \\text{Beta-Binomial}(n,\\alpha,\\beta)$.\n",
    "- $\\mathbb{E}[Y]=n \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "Variance is larger than Binomial variance (because it accounts for parameter uncertainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f5961",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back to the Gambler‚Äôs Success Proportion\n",
    "\n",
    "- In his first experiment, he wins 4 bets and loses 3.\n",
    "- In his second experiment, he wins 9 bets and loses 11.\n",
    "\n",
    "Let‚Äôs use a Beta prior with $\\alpha = 1$ and $\\beta = 1$.<br>\n",
    "Recall the Beta density: $P(p \\mid \\alpha, \\beta)=\\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)}$\n",
    "\n",
    "Substitute $\\alpha = 1$, $\\beta = 1$: $P(p \\mid \\alpha, \\beta)=\\frac{p^{1-1}(1-p)^{1-1}}{B(1,1)} = \\frac{p^0(1-p)^0}{1}=\\frac{1}{1} = 1$.\n",
    "- **Note**, this is probability density, not probability itself!\n",
    "- Intuitively this means, at first, all values of $p$ in $(0,1)$ are equally plausible (*a uniform distribution*).\n",
    "    - $\\mathbb{E}[p \\mid y]=\\frac{1}{1+1}=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df728116",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back to the Gambler‚Äôs Success Proportion\n",
    "\n",
    "After the first experiment we observe: $y = 4, \\quad N = 7$.<br>\n",
    "Using a $\\text{Beta}(1,1)$ prior: $\\alpha' = \\alpha + y = 1 + 4 = 5$, and, $\\beta' = \\beta + N - y = 1 + 7 - 4 = 4$.<br>\n",
    "So, the posterior distribution is $p \\mid y\\sim\\text{Beta}(5,4)$ with density $P(p \\mid \\alpha', \\beta') = \\frac{p^{5-1}(1-p)^{4-1}}{B(5,4)}$\n",
    "- $\\mathbb{E}[p \\mid y]=\\frac{5}{5+4} \\approx 0.556$\n",
    "- The numbers of successes and failures keep getting added to the parameters of the prior in each turn.\n",
    "\n",
    "As soon as we recognize that the distribution is another Beta distribution, we can:\n",
    "- calculate the expected value $\\frac{\\alpha'}{\\alpha'+\\beta'}$ and variance\n",
    "- can simulate experimental data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1bb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gamma‚ÄìPoisson Model\n",
    "\n",
    "<br>When Do We Use Gamma‚ÄìPoisson?</br>\n",
    "\n",
    "- The core concept is **waiting times** and the parameter of interest is a **rate parameter**, $\\lambda$.\n",
    "    - **Gamma Distribution**: Models the total time until the $n$-th event.\n",
    "    - **Poisson Distribution**: Flips the question‚Äîit models the number of events that occur in a fixed time.\n",
    "\n",
    "- Data are counts occurring in a fixed interval of time or space.\n",
    "- The likelihood is **Poisson**.\n",
    "- The conjugate prior (*and posterior*) is **Gamma**.\n",
    "- As the number of days ($n$) goes to infinity, the $\\alpha$ and $\\beta$ from your prior become mathematically irrelevant and expected value eventually converges to your observed one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68c903",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Setup\n",
    "\n",
    "Suppose we are interested in *the number of spam phone calls I receive per day*.\n",
    "\n",
    "- **Likelihood**\n",
    "    - $ Y \\mid \\lambda \\sim \\text{Poisson}(\\lambda) $\n",
    "    - For a single observation $y$: $P(y \\mid \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}$\n",
    "- **Prior**\n",
    "    - $\\lambda \\sim \\text{Gamma}(\\alpha, \\beta)$ \n",
    "    - $\\alpha$ roughly represents the number of events and $\\beta$ represents the interval/average time between events. \n",
    "    - $p(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\lambda^{\\alpha-1} e^{-\\beta \\lambda}$\n",
    "    - Mean: $\\mathbb{E}[\\lambda] = \\frac{\\alpha}{\\beta}$\n",
    "- **Posterior**\n",
    "    - Because Gamma is conjugate to Poisson: $\\lambda \\mid y \\sim \\text{Gamma}(\\alpha + y,\\; \\beta + 1)$\n",
    "    - For $n$ observations: $\\lambda \\mid y_1,\\dots,y_n \\sim \\text{Gamma}\\!\\left(\\alpha + \\sum y_i,\\; \\beta + n\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30d8fb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 1: Nearly Uninformed Prior\n",
    "\n",
    "Suppose I observe $y = 6$ spam calls in one day.\n",
    "- Prior: Smaller parameters are closer to *uninformed priors* \n",
    "    - $\\alpha = 0.001,\\; \\beta = 0.001$\n",
    "- Posterior parameters:\n",
    "    - $\\alpha' = \\alpha + y = 6.001$ and $\\beta' = \\beta + 1 = 1.001$\n",
    "- Posterior mean: almost entirely driven by the data!\n",
    "    - $\\mathbb{E}[\\lambda \\mid y] = \\frac{6.001}{1.001} \\approx 6.00$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01767d12",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 2: Very Opinionated Prior\n",
    "\n",
    "Suppose instead: $\\alpha = 50,\\quad \\beta = 2$\n",
    "- Prior mean: $\\mathbb{E}[\\lambda] = \\frac{50}{2} = 25$\n",
    "    - This prior strongly believes the rate is around 25 calls per day.\n",
    "- Posterior parameters: with $y = 6$\n",
    "    - $\\alpha' = 50 + 6 = 56$, and $\\beta' = 2 + 1 = 3$\n",
    "- Posterior mean: remains much closer to the prior belief than to the observed value.\n",
    "    - $\\mathbb{E}[\\lambda \\mid y]=\\frac{56}{3}\\approx 18.67$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca920c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normal-Normal Model\n",
    "\n",
    "- The Normal-Normal conjugate pair is used for continuous measurements $y$ that might reasonably follow a normal distribution.\n",
    "    - $\\text{Normal}(\\mu, \\sigma^2)$. We usually assume the population variance $\\sigma^2$ is known and we are trying to estimate the true mean $\\mu$.\n",
    "    - In practice, you may estimate variance using the standard unbiased estimator. \n",
    "        - $s^2 =\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}$\n",
    "\n",
    "- Suppose I am interested in whether the teaspoons I use to bake are accurate. A standard teaspoon should hold 4.2 grams.\n",
    "    - Then, the likelihood is the Normal distribution.\n",
    "    - The prior is the Normal distribution. The initial belief about $\\mu$ is $\\text{Normal}(\\mu_0, \\sigma^2)$\n",
    "- I weigh the amount my teaspoon can hold three times, and come up with measurements of $4.1$, $4.4$, and $4.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52954269",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concluding Thoughts\n",
    "\n",
    "- It‚Äôs totally fine to use a well known conjugate pair if it applies! Again, check the Wikipedia page for things you might be able to use, look at other examples similar to your context, etc.\n",
    "- Sometimes, we can make things slide easily out with nice, easy to update calculations.\n",
    "- What do we do when things are more complicated?\n",
    "    - The answer is something called a Markov Chain Monte Carlo simulation, but we need to review Markov Chains specifically first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
