{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52268dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "  \n",
    "  <div style=\"text-align: left;\">\n",
    "   <h2 style=\"font-size: 1.8em; margin-bottom: 0;\"><b>Branching out decisions in a tree</b></h2>\n",
    "   <br>\n",
    "   <h3 style=\" font-size: 1.2em;margin-bottom: 0;\">Decision Trees and Ensemble Learning</h3>\n",
    "   <h3 style=\"font-size: 1.2em; margin-bottom: 0; color: blue;\"><i>Dr. Satadisha Saha Bhowmick</i></h3>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"margin-right: 10px;\"> \n",
    "    <img src=\"media/images/dsi-logo-600.png\" align=\"right\" alt=\"UC-DSI\" scale=\"0.7;\">\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aadf503",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- ### Learning Loop -->\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: 5px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "    <h3>About Me</h3>\n",
    "  <h4>Satadisha Saha Bhowmick, Ph.D</h4>\n",
    "\n",
    "  <div class=\"fragment\"  style=\"font-size: 14px;\">\n",
    "    <h4>Affiliation</h4>\n",
    "    <ul>\n",
    "      <li>Postdoctoral Teaching Fellow <br> Data Science Institute, University of Chicago</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"  style=\"font-size: 14px;\">\n",
    "    <h4>Courses Sequences I teach</h4>\n",
    "    <ul>\n",
    "      <li>Introduction to Data Science</li>\n",
    "      <li>Mathematical Methods for Data Science</li>\n",
    "      <li>Ethics, Fairness, Responsibility, and Privacy in Data Science</li>\n",
    "      <li>Object Oriented Programming with Java</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"fragment\"  style=\"font-size: 14px;\">\n",
    "    <h4>Research Interest</h4>\n",
    "    <ul>\n",
    "      <li>Information Extraction</li>\n",
    "      <li>Short Text Mining</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  </div>\n",
    "  <div style=\"flex: 1;\">\n",
    "    <img src=\"media/images/satadisha-photo.png\" alt=\"Self\" scale=\"0.3\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8b9e3a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "from ipywidgets import interact\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.read_csv(\"data/data_for_tree_Oct22.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4d239",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's Learning Outcomes\n",
    "Course Module from DATA 119 Introduction to Data Science II\n",
    "\n",
    "<div style=\"display: flex; gap: 2px;\">\n",
    "\n",
    "  <div style=\"flex: 1;\">\n",
    "\n",
    "  <ul>\n",
    "    <li class=\"fragment\"> General understanding of Tree Models</li>\n",
    "    <li class=\"fragment\"> Data driven decision making with trees</li>\n",
    "    <li class=\"fragment\">Impurity functions to build decision boundaries for tree models.</li>\n",
    "  </ul>\n",
    "\n",
    "  \n",
    "  </div>\n",
    "\n",
    "  <div style=\"flex: 1;\">\n",
    "  <ul>\n",
    "    <li class=\"fragment\">Using an ensemble of tree-based learners</li> \n",
    "    <li class=\"fragment\"> Bagging</li>\n",
    "    <li class=\"fragment\"> Random Forest</li>\n",
    "  </ul>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3b784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setting The Scene\n",
    "\n",
    "<div style=\"display:flex; gap:20px;\">\n",
    "  <div style=\"flex:1;\">\n",
    "  <img src=\"media/images/bullet1.png\" alt=\"tab1\" scale=\"0.35;\" style=\"width: 20%;\">\n",
    "  <p>Most data that is interesting<br> enough for prediction has<br> some inherent structure.</p>\n",
    "  </div>\n",
    "  <div style=\"flex:1;\">\n",
    "  <img src=\"media/images/bullet2.png\" alt=\"tab1\" scale=\"0.35;\" style=\"width: 20%;\">\n",
    "  <p>Tree-based models exploit structure in data to split them into multiple homogenous subgroups</p>\n",
    "  <p>Approximates a (typically) discrete valued target function by repeatedly segmenting the predictor space into more homogeneous regions.</p>\n",
    "  <p>Represent a disjunction of conjunctions of constraints on the values of attributes representing the data.</p>\n",
    "  </div>\n",
    "  <div style=\"flex:1;\">\n",
    "  <img src=\"media/images/bullet3.png\" alt=\"tab1\" scale=\"0.35;\" style=\"width: 20%;\">\n",
    "  <p><b>Advantages</b></p>\n",
    "  <p>Training data need not be stored once the tree is constructed</p>\n",
    "  <p>Very fast during test time as test inputs only need to traverse down the tree to a leaf.</p>\n",
    "  <p>Decision trees require no distance metric because the splits are based on feature thresholds and not distances.</p>\n",
    "\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f279092",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree: Example\n",
    "\n",
    "- Assume a toy task that consisting of a dataset that contains several attributes related to trees growing in a plot of land. \n",
    "- Given only the $\\color{darkorange}{\\textbf{Diameter}}$ and $\\color{darkorange}{\\textbf{Height}}$ of a tree trunk, we must determine if it's an Apple, Cherry, or Oak tree. \n",
    "- To do this, we'll use a $\\color{darkorange}{\\textbf{Decision Tree}}$.\n",
    "\n",
    "<i>Let's start by investigating the data!</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501d8175",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree type</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cherry</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oak</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>Total</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tree type  Count\n",
       "0         apple     50\n",
       "1        cherry     50\n",
       "2           oak     50\n",
       "Total     Total    150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"Diameter\", \"Height\"]]\n",
    "print(\"Number of rows:\",len(data))\n",
    "\n",
    "#Number of instances per class\n",
    "class_counts = (\n",
    "    df[\"Family\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename_axis(\"Tree type\")\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "class_counts.loc[\"Total\"] = [\"Total\", class_counts[\"Count\"].sum()]\n",
    "class_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19fac6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"media/images/tree-data.png\" alt=\"Tree Data\" scale=\"0.55;\" style=\"width: 90%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d245b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Tree: Example\n",
    "\n",
    "Learned trees can also be thought of as <span style=\"color:blue;\"><i>sets of if-then rules</i></span> progressively dividing the feature space!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a2377",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"media/gif/decision_tree_growth.gif\" alt=\"Decision Tree Example\" scale=\"0.55;\" style=\"width: 90%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561887cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Tree: Example\n",
    "\n",
    "We can use the `DecisionTreeClassifier` module from `scikit-learn` with some added parameters to fit a full decision tree on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e91a9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Features and label\n",
    "X = df[[\"Diameter\", \"Height\"]].values\n",
    "y_raw = df[\"Family\"].astype(str).values\n",
    "\n",
    "# Encode class labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "class_names = list(le.classes_)\n",
    "\n",
    "# Fit a full (no max_depth cap) classification tree\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, random_state=0)\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3265d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Root Node</b></span>: Top of the tree, the whole sample is still together.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d31b74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Node</b></span>: Decision nodes that create splits.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed29b54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Child</b></span>: Nodes below the current node.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755937b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Parent</b></span>: Node above the current node.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b66f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Leaf/Terminal Node</b></span>: Nodes that have no children, predictions depend on data at these nodes.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8a1e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Trees: Terminology\n",
    "<span style=\"color:darkorange;\"><b>Pruning</b></span>: Removing unnecessary branches.\n",
    "<img src=\"media/images/full-DTree.png\" alt=\"Decision Tree\" scale=\"2.5\" style=\"height: 200%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65ddba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Trees: Algorithm\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: 20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "  <h4><span style=\"color:darkorange;\"><b>Top-Down Greedy Recursive Partitioning </b></span></h4>\n",
    "  <p>Choose the most efficient split locally that creates the most homogenous groups.</p>\n",
    "  <ul>\n",
    "    <li>Continuous and Discrete Features</li>\n",
    "    <li>Can split at the same attribute multiple times at different values.</li>\n",
    "    <li>Recursively keep splitting on each child node until a homogeneous label group is attained.</li>\n",
    "  </ul>\n",
    "  <h4><span style=\"color:darkorange;\"><b>Algorithms:</b></span></h4>\n",
    "  <ul>\n",
    "    <li>ID3 (Iterative Dichotomiser 3): <span style=\"color:blue;\"><u>Quinlan, 1986</u></span></li>\n",
    "    <li>C4.5 (successor of ID3)</li>\n",
    "    <li>CART (Classification And Regression Tree): <span style=\"color:blue;\"><u>Breiman, 1984</u></span></li>\n",
    "    <li>And more‚Ä¶</li>\n",
    "  </ul>\n",
    "\n",
    "  </div>\n",
    "  <div style=\"flex: 1;\">\n",
    "    <img src=\"media/images/table-ss.png\" alt=\"Decision Tree: Comparison\" scale=\"0.9\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc76070a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a Tree with an Impurity Function\n",
    "\n",
    "The split which results in node with minimal impurity is considered the optimal split.\n",
    "\n",
    "<img src=\"media/images/impurity-1.png\" alt=\"Decision Tree: Impurity\" scale=\"0.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18e6b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a Tree with Gini Impurity\n",
    "\n",
    "Range of values go from 0 for perfectly pure nodes to 0.5 for perfectly impure nodes.\n",
    "\n",
    "<img src=\"media/images/gini-impurity-1.png\" alt=\"Decision Tree: Impurity\" scale=\"0.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00693c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building a Tree with Gini Impurity\n",
    "\n",
    "Calculating the Gini Impurity on Numeric column like Age (that contains numbers and not just Yes/No values) is little more involved.\n",
    "\n",
    "<img src=\"media/images/gini-impurity-2.png\" alt=\"Decision Tree: Impurity\" scale=\"0.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78dd64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building a Tree with Gini Impurity\n",
    "\n",
    "All 3 possible first splits!\n",
    "\n",
    "<img src=\"media/images/gini-impurity-3.png\" alt=\"Decision Tree: Impurity\" scale=\"0.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c59f25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building a Tree with Gini Impurity\n",
    "\n",
    "Natural feature selection $\\rightarrow$ loves popcorn doesn‚Äôt appear in the full tree for the decision making process!\n",
    "\n",
    "<img src=\"media/images/full-tree-gini.png\" alt=\"Decision Tree: Impurity\" scale=\"0.9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0505e80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy as Impurity\n",
    "<ul>\n",
    "    <li><span style=\"color:darkorange;\"><b>Entropy</b></span> is a measure of <span style=\"color:darkorange;\">disorder</span>. The¬†entropy¬†of a¬†random variable is the average level of \"surprise\", or \"uncertainty\" inherent to the variable's possible outcomes.</li>\n",
    "        <ul>\n",
    "        <li> $E(S) = \\sum_{i=1}^{C}-p_ilog_2p_i$ </li>\n",
    "        <li> $p_i$ is the probability of class $i$. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> \n",
    "    Varies between <span style=\"color:darkorange;\"><b>0</b></span> and <span style=\"color:darkorange;\"><b>$log_2C$</b></span>.<br>\n",
    "    Worst case is a <span style=\"color:darkorange;\">uniform distribution of labels</span> ‚Äì all classes being equally likely. For binary classification $E(S) \\in (0,1)$\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938a162",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entropy as Impurity\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: 20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "  <p>In <span style=\"color:darkorange;\">Information Theory</span>: average number of bits needed to encode the classification of an example in $S$.</p>\n",
    "  <ul>\n",
    "    <li> If all examples are in the same class, receiver knows it‚Äôs always that class <span style=\"color:darkorange;\"><b>0 bit</b></span> (no information has to be sent)!</li>\n",
    "    <li> If both classes are equally likely, i.e. probability = 0.5, then we need <span style=\"color:darkorange;\"><b>1 bit</b></span>.</li>\n",
    "    <li>If one class has probability 0.8, on average it can be conveyed using shorter messages.</li>\n",
    "  </ul>\n",
    "  <p>I can start provisioning my bits in an optimal manner. $1^{st}$ bit to say red or not, add $2^{nd}$ bit only if it is blue, $3^{rd}$ bit if neither red not blue, and value of the bit tells us if it is green or yellow.</p>\n",
    "\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/entropy-calculation.png\" alt=\"Entropy\" scale=\"0.015\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00838cae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a Tree with Entropy\n",
    "\n",
    "- Entropy tells us how much information we need to define the uncertainty of a variable.\n",
    "    - <span style=\"color:darkorange;\">Low Entropy means more homogeneity! $\\rightarrow$ Can be used to quantify Impurity</span>\n",
    "- Our goal when building the Decision Tree is to reduce uncertainty or disorder with every split.\n",
    "- Mathematically this is quantified as <span style=\"color:darkorange;\"><b>Information Gain</b></span> $\\in (0,1)$.\n",
    "    - Expected reduction in entropy caused by partitioning the examples according to this attribute. \n",
    "    - It measures <i>how well</i> a given attribute separates the training examples according to their target classification.\n",
    "    - $IG(Y,X) = ùê∏(ùëå)‚àí ùê∏(ùëå|ùëã)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df1aa3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building a Tree with Entropy\n",
    "\n",
    "Here, we see part of a decision tree predicting if a person will be able to repay a loan.<br>\n",
    "Calculate the Entropy at the parent node and check how much uncertainty can be reduced upon splitting on the feature Balance.\n",
    "\n",
    "<ul>\n",
    "    <li>${\\scriptsize E(Parent) = -\\frac{16}{30}log_2\\frac{16}{30} - \\frac{14}{30}log_2\\frac{14}{30} = 0.99}$</li>\n",
    "    <li>${\\scriptsize E(Balance<50K) = -\\frac{12}{13}log_2\\frac{12}{13} - \\frac{1}{13}log_2\\frac{1}{13} = 0.39}$</li>\n",
    "    <li>${\\scriptsize E(Balance>50K) = -\\frac{4}{17}log_2\\frac{4}{17} - \\frac{13}{17}log_2\\frac{13}{17} = 0.79}$</li>\n",
    "</ul>\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "\n",
    "  <p>Weighted average of Entropy for splitting on Balance\n",
    "  $${\\scriptsize E(Balance) = \\frac{13}{30}0.39 + \\frac{17}{30}0.79 = 0.62}$$\n",
    "  </p>\n",
    "\n",
    "  <p>Information Gain through split\n",
    "  $${\\scriptsize IG(Parent, Balance) = E(Parent) - E(Balance) = 0.37}$$\n",
    "  </p>\n",
    "\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/entropy-node-split.png\" alt=\"Entropy\" scale=\"0.65\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bc831",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automatic Feature Selection\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "\n",
    "  <p>Even at impure nodes, we might not choose to split on an attribute if the impurity function is not minimized upon splitting the node as opposed to its original value.</p>\n",
    "\n",
    "  <p>Choosing not to split despite having attributes remaining for further split is a type of <span style=\"color:darkorange;\">automatic feature selection</span>.</p>\n",
    "\n",
    "  <p>Alternatively, instead of just checking for impurity reduction upon split, one can also set the threshold such that the reduction needs to exceed its value.</p>\n",
    "\n",
    "  <p>One way to limit <span style=\"color:darkorange;\">overfitting</span>!</p>\n",
    "\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/dt-feature-selection.png\" alt=\"Entropy\" width=\"85%\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a802b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Single trees overfit\n",
    "\n",
    "<span style=\"color:darkorange;\"><b>Decision Trees are prone to overfitting.</b></span><br> \n",
    "Without any additional oversight, CART and ID3 algorithms grows each branch of the tree just deeply enough to perfectly classify the training examples. \n",
    "\n",
    "Overfit trees are:\n",
    "- Too mindful to randomness of the training data\n",
    "- Compromises generalizability on unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70782073",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimize overfitting\n",
    "\n",
    "Must balance the depth and complexity of the tree to <b>generalize</b> to unseen data.\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "\n",
    "  <p>Two Main Options:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li>Early stopping</li>\n",
    "    <ul>\n",
    "    <li>Restrict tree depth</li>\n",
    "    <li>Restrict node size</li>\n",
    "    </ul>\n",
    "    <li>Pruning</li>\n",
    "  </ul>\n",
    "  <p>Training a full decision tree with $67\\%$ of initial data and testing on the rest yields an F1-score of $\\color{darkorange}{87\\%}$</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/full-decision-surface.png\" alt=\"DTSurface\" scale=\"0.65\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e8125",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Early stopping: <span style=\"color:darkorange;\">Limit tree depth</span>\n",
    "\n",
    "Stop splitting after a certain depth by setting `max_depth` parameter.\n",
    "```python\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "  <p>Training a decision tree of maximum depth 4 with $67\\%$ of initial data and testing on the rest yields an F1-score of $\\color{darkorange}{93\\%}$</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/decision-boundary-depth4.png\" alt=\"DTSurface4\" style=\"width:100%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23561e20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Early stopping: <span style=\"color:darkorange;\">Minimum node size</span>\n",
    "\n",
    "Do not split intermediate node which contains fewer than a minimum number of samples by setting `min_samples_split` parameter\n",
    "```python\n",
    "clf = DecisionTreeClassifier(min_samples_split=4)\n",
    "clf.fit(X, y)\n",
    "```\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\">\n",
    "  <p>Training a decision tree requiring a minimum of 4 samples in its internal node, with $67\\%$ of initial data and testing on the rest, yields an F1-score of $\\color{darkorange}{89\\%}$</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/decision-boundary-minsplit4.png\" alt=\"DTSurface4minsample\" style=\"width:100%;\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099af97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More on Decision Trees\n",
    "\n",
    "<img src=\"media/images/dt-pros-cons.png\" alt=\"Entropy\" scale=\"0.65\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38141808",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The problem with single trees\n",
    "- Single pruned trees are poor predictors\n",
    "- Single deep trees are noisy $\\rightarrow$ <b>very sensitive to small perturbations in data</b>.\n",
    "\n",
    "Only $5\\%$ Gaussian noise in the data changes the tree.\n",
    "<img src=\"media/images/dt-w-gaussian-noise.png\" alt=\"DTwGaussianNoise\" style=\"width:100%;\">\n",
    "\n",
    "<b>Fix<b>: <span style=\"color:darkorange;\">an ensemble of trees</span>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693bab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ensemble Learning\n",
    "An ML paradigm that combines multiple individual models to create a stronger and more robust model.\n",
    "\n",
    "<span style=\"color:darkorange;\">Bias Variance Decomposition</span> of Learning Error:\n",
    "${\\scriptsize E[(h_D(x)-y)^2] = \\underbrace{E[(h_D(x)-\\bar{h}(x))^2]}_{Variance}+ \\underbrace{E[(\\bar{h}(x)-\\bar{y}(x))^2]}_{Bias} + \\underbrace{E[(\\bar{y}(x)-y(x))^2]}_{Noise}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71258c2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ensemble Learning: Bagging\n",
    "- Reduces variance, so has a strong beneficial effect on high variance classifiers.\n",
    "- Train multiple instances of the same model on different subsets of the training data\n",
    "- Each subset is created by sampling with replacement - <span style=\"color:darkorange;\"><b>bootstrapping</b></span>!!\n",
    "- The final prediction is an average (for regression) or a majority vote (for classification) of the individual models.\n",
    "\n",
    "\n",
    "### Ensemble Learning: Boosting\n",
    "\n",
    "- Boosting focuses on sequentially improving the model's weaknesses.\n",
    "- It gives more weight to misclassified instances, allowing the model to learn from its mistakes.\n",
    "- Focuses on improving accuracy by systematically reducing <span style=\"color:darkorange;\">bias</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ed233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Downside of Decision Trees ‚Äì easily overfit and often inaccurate on new samples.<br>\n",
    "Goal: Reduce Variance term $E[(h_D(x)-\\bar{h}(x))^2]$ <br>\n",
    "To achieve this, we use Bagging. \n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\"> \n",
    "  <p>Inspired by the <span style=\"color:darkorange;\"><b>Weak law of large numbers</b></span>. Apply this to classifiers.</p>\n",
    "  <p>Assume we have $m$ training sets¬†$D_1,D_2,\\dots,D_m$ drawn from¬†population. Train a classifier on each one and average result:\n",
    "  $\\hat{h} = \\frac{1}{m}\\sum_{i=1}^mh_{D_i} \\rightarrow \\bar{h}$ as¬†$m \\rightarrow \\infty$.<br>\n",
    "  If $\\hat{h} \\rightarrow \\bar{h}$ the variance component of the error must also vanish. Problem is we don‚Äôt have m data sets, we only have one D.</p>\n",
    "\n",
    "  <span style=\"color:darkorange;\"><b>Solution</b></span>: Use bootstrapped samples!\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/bagging-fig.png\" alt=\"Random Forest\" style=\"width:90%;\">\n",
    "  </div>\n",
    "</div>\n",
    "    \n",
    "A popular bagged model with Decision Trees is called Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30db477",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bagging: Random Forest\n",
    "\n",
    "<div style=\"display: flex; align-items: center;gap: -20px;\">\n",
    "  <div style=\"flex: 1;\"> \n",
    "  <p>1. <span style=\"color:darkorange;\"><b>Bootstrapping</b></span>: Sample different subsets of data (sampling with replacement).</p>\n",
    "  <p>2. Builds multiple ‚Äòoverfit‚Äô decision trees and combines their predictions.</p>\n",
    "  <p>3. Random subsets of features are considered for splitting at each node to <span style=\"color:darkorange;\"><b>avoid tree correlation</b></span>. <span style=\"color:red;\"><b>Crucial!</b></span></p>\n",
    "  <p>4. The final prediction is an average (regression) or majority vote (classification) of individual tree predictions.</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; display: flex; flex-direction: column; align-items: center;\">\n",
    "    <img src=\"media/images/rf.png\" alt=\"Random Forest\" style=\"width:100%;\">\n",
    "    <div style=\"margin-top: 20px; color:darkorange; text-align: center;\">\n",
    "      Randomized feature selection with\n",
    "      Variance in Prediction\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15ffd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest: Example (Tree Data)\n",
    "Training a full decision tree with $67\\%$ of initial data and testing on the rest yields an F1-score of $\\color{darkorange}{91\\%}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788fe11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,     # number of trees\n",
    "    max_depth=None,      # grow full trees\n",
    "    min_samples_split=2,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score (weighted): {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba88967",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation of a Random Forest\n",
    "- <span style=\"color:darkorange;\"><b>Out-of-Bag Data</b></span>: Entries that did not make into a bootstrapped dataset.\n",
    "    - For large enough N, on average, 63.21% or the original records end up in any bootstrap sample.\n",
    "    - Roughly 36.79% of the observations are not used in the construction of a particular tree.\n",
    "- An out-of-bag sample can be used to test all the trees in the Random Forest trained without it.\n",
    "- <span style=\"color:darkorange;\"><b>Free cross-validation</b></span>: Random Forest accuracy can be gauged by the proportion of out-of-bag samples that were correctly classified by the forest.\n",
    "- The proportion of out-of-bag samples that were incorrectly classified is the <span style=\"color:darkorange;\"><b>Out-of-Bag Error</b></span>. \n",
    "    - This is an unbiased estimate of the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8dceb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest for Classifying Digits\n",
    "\n",
    "We want to use Random Forest to classify hand-written digits. Our dataset consists of 1797 grayscale images of handwritten digits, from 0 to 9. Each image is an $8 \\times 8$ pixel grid that has been flattened into a vector of 64 numerical features.\n",
    "\n",
    "We use separate out $25\\%$ of the data for evaluation and train the model on the rest.\n",
    "<img src=\"media/images/digits-data.png\" alt=\"Digits Data\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe4757d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "digits = load_digits()\n",
    "digits.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d960128",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        38\n",
      "           1       0.98      0.95      0.97        44\n",
      "           2       0.95      1.00      0.98        42\n",
      "           3       0.98      0.98      0.98        45\n",
      "           4       0.97      1.00      0.99        37\n",
      "           5       0.98      0.96      0.97        49\n",
      "           6       1.00      1.00      1.00        52\n",
      "           7       1.00      0.96      0.98        50\n",
      "           8       0.94      0.98      0.96        46\n",
      "           9       0.98      0.98      0.98        47\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
    "                                                random_state=0)\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)\n",
    "print(classification_report(ypred, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056832d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Out-of-Bag Error\n",
    "\n",
    "Although for smaller samples (small $N$), OOB is less reliable than validation, as $N$ increases OOB  is more efficient than cross-validation.<br>\n",
    "OOB error and Test errors both stabilize with the number of trees.\n",
    "\n",
    "Albeit a lower test error is a bit lucky! $\\downarrow$\n",
    "\n",
    "<img src=\"media/images/rf-oob-vs-test.png\" alt=\"Digits Data- OOB v Test\" style=\"width:70%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236a479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rounding up our thoughts\n",
    "\n",
    "<div style=\"display: flex; gap: 2px;\">\n",
    "  <div style=\"flex: 1;\"> \n",
    "  <p><span style=\"color:darkorange;\"><b>What did we learn?</b></span></p>\n",
    "  <ul>\n",
    "  <li>Decision Trees</li>\n",
    "  <ul>\n",
    "    <li>Confident and Fast</li>\n",
    "    <li>Good at learning structures within feature space</li>\n",
    "    <li>But alone it overfits</li>\n",
    "  </ul>\n",
    "  <li>Bagging</li>\n",
    "  <ul>\n",
    "    <li>Wisdom of the crowd!</li>\n",
    "  </ul>\n",
    "  </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1;\">\n",
    "  <p><span style=\"color:darkorange;\"><b>What more on Decision Trees?</b></span></p>\n",
    "  <ul>\n",
    "    <li>A closer look at Regression Trees</li> \n",
    "    <li> Minimize Overfitting in Single Trees w Reduced Error Pruning</li>\n",
    "    <li> Boosting</li>\n",
    "    <ul>\n",
    "    <li> AdaBoost</li>\n",
    "    <li> Gradient Boosting with Regression Trees</li>\n",
    "    <li> XGBoost for Classification and Regression</li>\n",
    "    </ul>\n",
    "  </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17285c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thats all folks!\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"media/images/end-slide.jpg\" alt=\"The End\" scale=\"0.01;\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
